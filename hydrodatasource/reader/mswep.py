"""
Author: Wenyu Ouyang
Date: 2024-01-03 15:40:58
LastEditTime: 2025-03-19 16:55:02
LastEditors: Wenyu Ouyang
Description: Reading MSWEP data
FilePath: \hydrodatasource\hydrodatasource\reader\mswep.py
Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.
"""

import os
from collections import OrderedDict
import glob
import pandas as pd
from pathlib import Path
import numpy as np
import xarray as xr
from tqdm import tqdm

from hydrodatasource.reader.data_source import HydroData


class Mswep(HydroData):
    """Reading MSWEP precipitation data."""

    def __init__(self, data_path):
        """åˆå§‹åŒ–MSWEPæ•°æ®è¯»å–å™¨

        Parameters
        ----------
        data_path : str
            MSWEPæ•°æ®çš„æ ¹ç›®å½•è·¯å¾„
        """
        self.data_source_dir = data_path
        self.data_source_description = self.set_data_source_describe()

    def get_name(self):
        return "MSWEP"

    def set_data_source_describe(self):
        """è®¾ç½®æ•°æ®æºæè¿°ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ•°æ®ç›®å½•ç»“æ„"""
        data_root_dir = self.data_source_dir
        periods = ["NRT", "Past"]
        time_resolutions = ["Daily", "3hourly", "Monthly"]

        data_description = {}
        for period in periods:
            period_dir = os.path.join(data_root_dir, period)
            data_description[period] = {}
            for resolution in time_resolutions:
                res_dir = os.path.join(period_dir, resolution)
                data_description[period][resolution] = res_dir

        return OrderedDict(
            DATA_DIR=data_root_dir,
            PERIOD_DATA=data_description,
        )

    def check_data_source(self):
        """This is a one-time check for the integrity of the data source.
        Please run it when you first use the data source.

        Raises
        ------
        ValueError
            å¦‚æœå­˜åœ¨ä¸å¯è¯»æ–‡ä»¶æˆ–å˜é‡ç¼ºå¤±
        """
        error_files = []
        period_data = self.data_source_description["PERIOD_DATA"]

        # é¢„æ‰«ææ‰€æœ‰æ–‡ä»¶è·¯å¾„
        print("ğŸ•µ æ‰«ææ–‡ä»¶ç›®å½•ç»“æ„...")
        all_nc_files = []
        for period in period_data:
            for time_res in period_data[period]:
                data_dir = period_data[period][time_res]
                if not os.path.isdir(data_dir):
                    continue
                nc_files = glob.glob(os.path.join(data_dir, "*.nc"))
                all_nc_files.extend(nc_files)

        # åˆ›å»ºè¿›åº¦æ¡
        with tqdm(
            total=len(all_nc_files),
            unit="file",
            desc="ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶",
            dynamic_ncols=True,
        ) as pbar:
            for nc_file in all_nc_files:
                try:
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_postfix(file=f"{nc_file}...")

                    # æ£€æŸ¥æ–‡ä»¶å¯è¯»æ€§
                    with xr.open_dataset(nc_file) as ds:
                        _ = ds["precipitation"].values[0]  # ä»…æ£€æŸ¥é¦–å€¼

                    # è¿›åº¦æ¡é¢œè‰²æ ‡è®°
                    pbar.set_postfix(status="âœ…")
                except Exception as e:
                    error_files.append(nc_file)
                    pbar.set_postfix(status="âŒ")
                    tqdm.write(f"ERROR: {nc_file} â€”â€” {str(e)[:50]}...")
                finally:
                    pbar.update(1)

        # ç»“æœæ±‡æ€»
        if error_files:
            error_rate = len(error_files) / len(all_nc_files) * 100
            err_msg = (
                f"æ•°æ®æ£€æŸ¥æœªé€šè¿‡ï¼é”™è¯¯ç‡ {error_rate:.2f}%\næŸåæ–‡ä»¶ç¤ºä¾‹:\n"
                + "\n".join(error_files[:3])
            )
            if len(error_files) > 3:
                err_msg += f"\n...ç­‰å…± {len(error_files)} ä¸ªé”™è¯¯æ–‡ä»¶"
            raise ValueError(err_msg)
        print(f"âœ… å…¨éƒ¨ {len(all_nc_files)} ä¸ªæ–‡ä»¶æ£€æŸ¥é€šè¿‡")

    def read_ts_xrdataset(
        self,
        time_range: list,
        period: str = "Past",
        time_resolution: str = "Daily",
        bbox: list = None,
        **kwargs,
    ) -> xr.Dataset:
        """ä¼˜åŒ–åçš„MSWEPæ•°æ®è¯»å–æ–¹æ³•"""
        # å‚æ•°éªŒè¯å’Œæ—¶é—´è§£æ
        if period not in ["NRT", "Past"]:
            raise ValueError("periodå¿…é¡»ä¸º'NRT'æˆ–'Past'")
        if time_resolution not in ["Daily", "3hourly", "Monthly"]:
            raise ValueError("æ—¶é—´åˆ†è¾¨ç‡å¿…é¡»ä¸º'Daily', '3hourly'æˆ–'Monthly'")

        start_dt = pd.to_datetime(time_range[0]).tz_localize(None)  # è½¬æ¢ä¸ºæ— æ—¶åŒºæ—¶é—´
        end_dt = pd.to_datetime(time_range[1]).tz_localize(None)

        # è·å–æ•°æ®ç›®å½•
        data_dir = self.data_source_description["PERIOD_DATA"][period][time_resolution]

        # ç”Ÿæˆéœ€è¦åŒ¹é…çš„æ–‡ä»¶åæ¨¡å¼
        file_patterns = self._generate_file_patterns(start_dt, end_dt, time_resolution)

        # æ”¶é›†åŒ¹é…çš„æ–‡ä»¶
        matched_files = []
        for pattern in file_patterns:
            full_pattern = str(Path(data_dir) / pattern)
            matched_files.extend(glob.glob(full_pattern))

        matched_files = sorted(list(set(matched_files)))  # å»é‡å¹¶æ’åº

        if not matched_files:
            raise FileNotFoundError(
                f"åœ¨{data_dir}ä¸­æœªæ‰¾åˆ°{start_dt}åˆ°{end_dt}æœŸé—´çš„æ•°æ®æ–‡ä»¶"
            )

        # æ™ºèƒ½åˆå¹¶æ•°æ®é›†
        ds = self._smart_merge(matched_files, time_resolution)

        # ç²¾ç¡®æ—¶é—´ç­›é€‰
        ds = ds.sel(time=slice(start_dt, end_dt))

        # ç©ºé—´è£å‰ªä¼˜åŒ–
        if bbox:
            ds = self._optimize_spatial_slice(ds, bbox)

        return ds

    def _generate_file_patterns(self, start_dt, end_dt, time_res):
        """æ ¹æ®æ—¶é—´èŒƒå›´å’Œåˆ†è¾¨ç‡ç”Ÿæˆæ–‡ä»¶ååŒ¹é…æ¨¡å¼"""
        patterns = []
        delta = pd.Timedelta(days=1)

        # ç”Ÿæˆæ—¥æœŸåºåˆ—
        date_seq = pd.date_range(start_dt.floor("D"), end_dt.ceil("D"), freq=delta)

        for dt in date_seq:
            year = dt.year
            doy = dt.dayofyear  # å¹´åºæ—¥

            if time_res == "Daily":
                # æ—¥æ•°æ®ï¼šYYYYddd.nc å¦‚2000001.nc
                patterns.append(f"{year}{doy:03d}.nc")
            elif time_res == "3hourly":
                # 3å°æ—¶æ•°æ®ï¼šYYYYddd.HH.nc å¦‚2000001.03.nc
                for hour in [0, 3, 6, 9, 12, 15, 18, 21]:
                    patterns.append(f"{year}{doy:03d}.{hour:02d}.nc")
            elif time_res == "Monthly":
                # æœˆæ•°æ®ï¼šYYYYMM.nc å¦‚200001.nc
                patterns.append(f"{year}{dt.month:02d}.nc")

        return list(set(patterns))  # å»é‡

    def _smart_merge(self, file_list, time_res):
        """æ™ºèƒ½åˆå¹¶ç­–ç•¥"""
        # æ„å»ºæ—¶é—´ç´¢å¼•æ˜ å°„
        time_index = []
        for f in file_list:
            fname = Path(f).stem
            if time_res == "Daily":
                time_str = fname[:7]
                year = int(time_str[:4])
                doy = int(time_str[4:])
                dt = pd.Timestamp(year=year, month=1, day=1) + pd.Timedelta(
                    days=doy - 1
                )
            elif time_res == "3hourly":
                time_str, hour = fname.split(".")
                year = int(time_str[:4])
                doy = int(time_str[4:7])
                base_dt = pd.Timestamp(year=year, month=1, day=1) + pd.Timedelta(
                    days=doy - 1
                )
                dt = base_dt + pd.Timedelta(hours=int(hour))
            elif time_res == "Monthly":
                year = int(fname[:4])
                month = int(fname[4:6])
                dt = pd.Timestamp(year=year, month=month, day=1)

            time_index.append(dt)

        # åˆ›å»ºè™šæ‹Ÿæ—¶é—´ç´¢å¼•
        # time_coord = xr.Coordinates(
        #     "time", pd.DatetimeIndex(time_index), {"long_name": "time"}
        # )

        # å¹¶è¡Œè¯»å–ä¼˜åŒ–
        ds = xr.open_mfdataset(
            file_list,
            combine="nested",
            concat_dim="time",
            parallel=True,
            chunks={"time": 100},
            decode_times=False,
        )

        # é‡å»ºæ­£ç¡®çš„æ—¶é—´åæ ‡
        ds["time"] = time_index
        return ds

    def _optimize_spatial_slice(self, ds, bbox):
        """ä¼˜åŒ–ç©ºé—´è£å‰ª"""
        lon_min, lat_min, lon_max, lat_max = bbox
        # ç¡®ä¿ç»åº¦åœ¨[-180, 180]èŒƒå›´
        ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))
        # å¿«é€Ÿç©ºé—´ç´¢å¼•
        return ds.sel(
            lon=slice(np.floor(lon_min), np.ceil(lon_max)),
            lat=slice(np.floor(lat_max), np.ceil(lat_min)),  # çº¬åº¦é™åº
        ).interp(
            lon=np.arange(np.floor(lon_min), np.ceil(lon_max) + 0.1, 0.1),
            lat=np.arange(np.floor(lat_min), np.ceil(lat_max) + 0.1, 0.1)[::-1],
        )


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    data_dir = r"D:\data\MSWEP_V280"
    mswep = Mswep(data_dir)
    # try:
    #     mswep.check_data_source()
    # except ValueError as e:
    #     print(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æœªé€šè¿‡:\n{e}")
    # è¯»å–2010å¹´1æœˆçš„æ•°æ®
    ds = mswep.read_ts_xrdataset(
        time_range=["2010-01-01", "2010-01-31"],
        period="Past",
        time_resolution="Daily",
        bbox=[100, 20, 120, 40],  # ç¤ºä¾‹ç©ºé—´èŒƒå›´
    )
    print(ds)
