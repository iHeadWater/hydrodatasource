"""
Author: Wenyu Ouyang
Date: 2025-01-19 18:05:00
LastEditTime: 2025-07-17 16:22:06
LastEditors: Wenyu Ouyang
Description: æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±» - ç»§æ‰¿è‡ªSelfMadeHydroDataset
FilePath: /hydrodatasource/hydrodatasource/reader/floodevent.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import pandas as pd
import numpy as np
import os
from typing import List, Dict, Optional, Tuple
from hydrodatasource.configs.data_consts import FLOOD_EVENT_VARS
from hydrodatasource.utils.utils import streamflow_unit_conv
from hydrodatasource.reader.data_source import SelfMadeHydroDataset


class FloodEventDatasource(SelfMadeHydroDataset):
    """
    æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±»

    ç»§æ‰¿è‡ªSelfMadeHydroDatasetï¼Œä¸“é—¨ç”¨äºå¤„ç†åˆ°é€ä¸ªæ´ªæ°´åœºæ¬¡æ•°æ®ï¼Œ
    åŒ…æ‹¬è¯»å–æµåŸŸé¢ç§¯ã€å•ä½è½¬æ¢ã€åœºæ¬¡æå–ç­‰åŠŸèƒ½ã€‚
    """

    def __init__(
        self,
        data_path: str,
        dataset_name: str = "songliaorrevents",
        time_unit: Optional[List[str]] = None,
        flow_unit: str = "mm/3h",
        **kwargs,
    ):
        """
        åˆå§‹åŒ–æµåŸŸåœºæ¬¡æ•°æ®é›†

        Args:
            data_path: æ•°æ®è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
            time_unit: æ—¶é—´å•ä½åˆ—è¡¨ï¼Œé»˜è®¤ä¸º["3h"]
            flow_unit: å¾„æµå•ä½ï¼Œé»˜è®¤ä¸º"mm/3h"
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        if time_unit is None:
            time_unit = ["3h"]
        # sometimes we load the data with different flow unit
        # so we need to store the flow unit
        self.flow_unit = flow_unit
        super().__init__(
            data_path=data_path,
            download=False,
            time_unit=time_unit,
            dataset_name=dataset_name,
            **kwargs,
        )

    def extract_flood_events(
        self, df: pd.DataFrame
    ) -> List[Tuple[np.ndarray, np.ndarray, str]]:
        """
        ä»æ•°æ®æ¡†ä¸­æå–æ´ªæ°´äº‹ä»¶ï¼Œè¿”å›å‡€é›¨ã€å¾„æµæ•°ç»„å’Œæ´ªå³°æ—¥æœŸ

        Args:
            df: ç«™ç‚¹æ•°æ®æ¡†
            station_id: ç«™ç‚¹IDï¼ˆç”¨äºæ‰“å°ä¿¡æ¯ï¼‰

        Returns:
            List[Tuple[np.ndarray, np.ndarray, str]]: (å‡€é›¨æ•°ç»„, å¾„æµæ•°ç»„, æ´ªå³°æ—¥æœŸ) åˆ—è¡¨
        """
        events = []
        # æ‰¾åˆ°è¿ç»­çš„flood_event > 0åŒºé—´
        flood_mask = df["flood_event"] > 0

        if not flood_mask.any():
            return events

        # æ‰¾è¿ç»­åŒºé—´
        in_event = False
        start_idx = None

        for idx, is_flood in enumerate(flood_mask):
            if is_flood and not in_event:
                start_idx = idx
                in_event = True
            elif not is_flood and in_event:
                # äº‹ä»¶ç»“æŸï¼Œæå–æ•°æ®
                event_data = df.iloc[start_idx:idx]
                net_rain = event_data["net_rain"].values
                inflow = event_data["inflow"].values
                event_times = event_data["time"].values

                # åŸºæœ¬éªŒè¯
                if len(net_rain) > 0 and len(inflow) > 0 and np.nansum(inflow) > 1e-6:
                    # è·å–åœºæ¬¡å¼€å§‹å’Œç»“æŸæ—¶é—´
                    start_time = event_times[0]
                    end_time = event_times[-1]

                    # è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ (YYYYMMDDHH)
                    def time_to_ten_digits(time_obj):
                        """å°†æ—¶é—´å¯¹è±¡è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ YYYYMMDDHH"""
                        if isinstance(time_obj, np.datetime64):
                            # å¦‚æœæ˜¯numpy datetime64å¯¹è±¡
                            return (
                                time_obj.astype("datetime64[h]")
                                .astype(str)
                                .replace("-", "")
                                .replace("T", "")
                                .replace(":", "")
                            )
                        elif hasattr(time_obj, "strftime"):
                            # å¦‚æœæ˜¯datetimeå¯¹è±¡
                            return time_obj.strftime("%Y%m%d%H")
                        else:
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                            try:
                                from datetime import datetime

                                if isinstance(time_obj, str):
                                    dt = datetime.fromisoformat(
                                        time_obj.replace("Z", "+00:00")
                                    )
                                    return dt.strftime("%Y%m%d%H")
                                else:
                                    return "0000000000"  # é»˜è®¤å€¼
                            except:
                                return "0000000000"  # é»˜è®¤å€¼

                    start_digits = time_to_ten_digits(start_time)
                    end_digits = time_to_ten_digits(end_time)

                    # ç»„åˆæˆåœºæ¬¡åç§°ï¼šèµ·å§‹æ—¶é—´_ç»“æŸæ—¶é—´
                    event_name = f"{start_digits}_{end_digits}"

                    events.append((net_rain, inflow, event_name))

                in_event = False
        return events

    def create_event_dict(
        self,
        net_rain: np.ndarray,
        inflow: np.ndarray,
        event_name: str,
        include_peak_obs: bool = True,
    ) -> Optional[Dict]:
        """
        å°†å‡€é›¨å’Œå¾„æµæ•°ç»„è½¬æ¢ä¸ºæ ‡å‡†äº‹ä»¶å­—å…¸æ ¼å¼

        Parameters
        ----------
        net_rain: np.ndarray
            å‡€é›¨æ•°ç»„
        inflow: np.ndarray
            å¾„æµæ•°ç»„
        event_name: str
            æ´ªå³°æ—¥æœŸï¼ˆ8ä½æ•°å­—æ ¼å¼ï¼‰
        include_peak_obs: bool
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼

        Returns
        -------
            Dict: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸ï¼Œä¸uh_utils.pyå®Œå…¨å…¼å®¹
        """
        try:
            # è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            valid_rain_mask = ~np.isnan(net_rain) & (net_rain > 0)
            m_eff = np.sum(valid_rain_mask)

            if m_eff == 0:
                return None

            # éªŒè¯å¾„æµæ•°æ®
            if np.nansum(inflow) < 1e-6:
                return None

            # åˆ›å»ºæ ‡å‡†æ ¼å¼å­—å…¸ï¼ˆä¸uh_utils.pyæœŸæœ›çš„keyå®Œå…¨ä¸€è‡´ï¼‰
            event_dict = {
                FLOOD_EVENT_VARS["NET_RAIN"]: net_rain,  # æœ‰æ•ˆé™é›¨ï¼ˆå‡€é›¨ï¼‰
                FLOOD_EVENT_VARS["OBS_FLOW"]: inflow,  # è§‚æµ‹å¾„æµ
                "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
                "n_specific": len(net_rain),  # å•ä½çº¿é•¿åº¦
                "filepath": f"event_{event_name}.csv",  # æ·»åŠ filepathå­—æ®µé¿å…KeyError
            }

            # æ·»åŠ æ´ªå³°è§‚æµ‹å€¼
            if include_peak_obs:
                peak_flow = np.nanmax(inflow)
                if peak_flow < 1e-6:
                    return None
                event_dict["peak_obs"] = peak_flow

            return event_dict

        except Exception:
            return None

    def _load_1basin_flood_events(
        self,
        station_id: Optional[str] = None,
        include_peak_obs: bool = True,
        verbose: bool = True,
    ) -> Optional[List[Dict]]:
        """
        åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®

        Parameters
        ----------
        station_id:
            æŒ‡å®šç«™ç‚¹IDï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç«™ç‚¹
        include_peak_obs:
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose:
            æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns
        -------
            List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰ç®—æ³•å®Œå…¨å…¼å®¹
        """
        # è·å–æµåŸŸé¢ç§¯
        basin_area_km2 = None
        if station_id:
            try:
                basin_area_km2 = self.read_area([station_id])
                if verbose:
                    print(f"ğŸ“Š è¯»å–åˆ°æµåŸŸé¢ç§¯: {basin_area_km2} kmÂ²")
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ æ— æ³•è¯»å–æµåŸŸé¢ç§¯: {str(e)}")

        try:
            if verbose:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®...")
                if station_id:
                    print(f"   æŒ‡å®šç«™ç‚¹: {station_id}")

            all_events = []
            total_events = 0

            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=["1960-01-01", "2024-12-31"],
                var_lst=["inflow", "net_rain", "flood_event"],
                # recache=True,
            )["3h"]
            if self.flow_unit == "mm/3h":
                xr_ds["inflow"] = streamflow_unit_conv(
                    xr_ds[["inflow"]], basin_area_km2, target_unit="mm/3h"
                )["inflow"]
            elif self.flow_unit == "m^3/s":
                pass
            else:
                raise ValueError(f"Unsupported flow unit: {self.flow_unit}")
            df = xr_ds.to_dataframe()
            if df is None:
                return None

            # æå–æ´ªæ°´äº‹ä»¶
            flood_events = self.extract_flood_events(df.loc[station_id].reset_index())

            if not flood_events:
                if verbose:
                    print(f"  âš ï¸  {station_id}: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ´ªæ°´äº‹ä»¶")
                return None

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            station_event_count = 0
            for net_rain, inflow, event_name in flood_events:
                event_dict = self.create_event_dict(
                    net_rain, inflow, event_name, include_peak_obs
                )
                if event_dict is not None:
                    all_events.append(event_dict)
                    station_event_count += 1

            if verbose and station_event_count > 0:
                print(f"  âœ… {station_id}: æˆåŠŸå¤„ç† {station_event_count} ä¸ªæ´ªæ°´äº‹ä»¶")
                total_events += station_event_count

            if not all_events:
                if verbose:
                    print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ´ªæ°´äº‹ä»¶æ•°æ®")
                return None

            if verbose:
                print(f"âœ… æ€»å…±æˆåŠŸåŠ è½½ {len(all_events)} ä¸ªæ´ªæ°´äº‹ä»¶")

            return all_events

        except Exception as e:
            if verbose:
                print(f"âŒ åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None


def _calculate_event_characteristics(event: Dict, delta_t_hours: float = 3.0) -> Dict:
    """
    è®¡ç®—æ´ªæ°´äº‹ä»¶çš„è¯¦ç»†ç‰¹å¾æŒ‡æ ‡ï¼Œç”¨äºç”»å›¾å’Œåˆ†æ

    Parameters
    ----------
        event: dict
            äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« 'P_eff' (å‡€é›¨) å’Œ 'Q_obs_eff' (å¾„æµ) æ•°ç»„
        delta_t_hours: float
            æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns
    -------
        Dict: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡

    Calculated metrics:
        - peak_obs: æ´ªå³°æµé‡ (mÂ³/s)
        - runoff_volume_m3: æ´ªé‡ (mÂ³)
        - runoff_duration_hours: æ´ªæ°´å†æ—¶ (å°æ—¶)
        - total_net_rain: æ€»å‡€é›¨é‡ (mm)
        - lag_time_hours: æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
    """
    try:
        # æå–æ•°æ®
        net_rain = event.get(FLOOD_EVENT_VARS["NET_RAIN"], [])
        direct_runoff = event.get(FLOOD_EVENT_VARS["OBS_FLOW"], [])

        net_rain = np.array(net_rain)
        direct_runoff = np.array(direct_runoff)

        # è½¬æ¢ä¸ºç§’
        delta_t_seconds = delta_t_hours * 3600.0

        # 1. è®¡ç®—æ´ªå³°æµé‡
        peak_obs = np.max(direct_runoff)
        if peak_obs < 1e-6:
            return None

        # 2. è®¡ç®—æ´ªé‡ (mÂ³)
        runoff_volume_m3 = np.sum(direct_runoff) * delta_t_seconds

        # 3. è®¡ç®—æ´ªæ°´å†æ—¶ (å°æ—¶)
        runoff_indices = np.where(direct_runoff > 1e-6)[0]
        if len(runoff_indices) < 2:
            return None
        runoff_duration_hours = (
            runoff_indices[-1] - runoff_indices[0] + 1
        ) * delta_t_hours

        # 4. è®¡ç®—æ€»å‡€é›¨é‡ (mm)
        total_net_rain = np.sum(net_rain)

        # 5. è®¡ç®—æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
        t_peak_flow_idx = np.argmax(direct_runoff)
        t_peak_rain_idx = np.argmax(net_rain)
        lag_time_hours = (t_peak_flow_idx - t_peak_rain_idx) * delta_t_hours

        # 6. è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
        m_eff = len(net_rain)

        # 7. è®¡ç®—å¾„æµæ—¶æ®µæ•°
        n_obs = len(direct_runoff)

        # 8. è®¡ç®—å•ä½çº¿é•¿åº¦
        n_specific = n_obs - m_eff + 1

        # è¿”å›è®¡ç®—ç»“æœ
        characteristics = {
            "peak_obs": peak_obs,  # æ´ªå³°æµé‡ (mÂ³/s)
            "runoff_volume_m3": runoff_volume_m3,  # æ´ªé‡ (mÂ³)
            "runoff_duration_hours": runoff_duration_hours,  # æ´ªæ°´å†æ—¶ (å°æ—¶)
            "total_net_rain": total_net_rain,  # æ€»å‡€é›¨é‡ (mm)
            "lag_time_hours": lag_time_hours,  # æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
            "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            "n_obs": n_obs,  # å¾„æµæ—¶æ®µæ•°
            "n_specific": n_specific,  # å•ä½çº¿é•¿åº¦
            "delta_t_hours": delta_t_hours,  # æ—¶æ®µé•¿åº¦
        }

        return characteristics

    except Exception as e:
        print(f"è®¡ç®—äº‹ä»¶ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return None


def calculate_events_characteristics(
    events: List[Dict], delta_t_hours: float = 3.0
) -> List[Dict]:
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªæ´ªæ°´äº‹ä»¶çš„ç‰¹å¾æŒ‡æ ‡

    Args:
        events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« 'P_eff' å’Œ 'Q_obs_eff' æ•°ç»„
        delta_t_hours: æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns:
        List[Dict]: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡çš„äº‹ä»¶åˆ—è¡¨
    """
    enhanced_events = []

    for i, event in enumerate(events):
        # è®¡ç®—ç‰¹å¾æŒ‡æ ‡
        characteristics = _calculate_event_characteristics(event, delta_t_hours)

        if characteristics is not None:
            # å°†ç‰¹å¾æŒ‡æ ‡æ·»åŠ åˆ°åŸäº‹ä»¶å­—å…¸ä¸­
            enhanced_event = event.copy()
            enhanced_event.update(characteristics)
            enhanced_events.append(enhanced_event)
        else:
            print(f"âš ï¸ äº‹ä»¶ {i+1} ç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡")

    return enhanced_events


def load_and_preprocess_events_unified(
    data_dir: str,
    station_id: Optional[str] = None,
    include_peak_obs: bool = True,
    verbose: bool = True,
    flow_unit: str = "mm/3h",
) -> Optional[List[Dict]]:
    """
    å‘åå…¼å®¹çš„ç»Ÿä¸€æ¥å£å‡½æ•°

    Args:
        data_source: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        station_id: æµåŸŸç«™ç‚¹IDï¼ˆå¯é€‰ï¼‰
        include_peak_obs: æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        recache: æ˜¯å¦é‡æ–°ç¼“å­˜æ•°æ®ï¼Œé»˜è®¤ä¸ºFalse

    Returns:
        List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰å•ä½çº¿ç®—æ³•å®Œå…¨å…¼å®¹
    """
    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    dataset = FloodEventDatasource(
        data_dir,
        flow_unit=flow_unit,
        trange4cache=["1960-01-01 02", "2024-12-31 23"],
    )
    return dataset._load_1basin_flood_events(station_id, include_peak_obs, verbose)


def check_event_data_nan(all_event_data: List[Dict]):
    """
    æ£€æŸ¥æ‰€æœ‰æ´ªæ°´äº‹ä»¶æ•°æ®ä¸­çš„é™é›¨å’Œå¾„æµæ˜¯å¦æœ‰ç©ºå€¼ï¼Œè‹¥æœ‰åˆ™æŠ¥é”™å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯ã€‚
    Args:
        all_event_data: äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå­—å…¸åŒ…å«P_effã€Q_obs_effã€filepathç­‰ï¼‰
    Raises:
        ValueError: å¦‚æœå‘ç°ç©ºå€¼ï¼ŒæŠ›å‡ºå¼‚å¸¸å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    for event in all_event_data:
        event_name = event.get("filepath", "unknown")
        p_eff = event.get(FLOOD_EVENT_VARS["NET_RAIN"])
        q_obs = event.get(FLOOD_EVENT_VARS["OBS_FLOW"])
        # æ£€æŸ¥é™é›¨
        if p_eff is not None and np.any(np.isnan(p_eff)):
            nan_idx = np.where(np.isnan(p_eff))[0]
            print(f"âŒ åœºæ¬¡ {event_name} çš„ P_eff å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}")
            raise ValueError(f"Event {event_name} has NaN in P_eff at index {nan_idx}")
        # æ£€æŸ¥å¾„æµ
        if q_obs is not None and np.any(np.isnan(q_obs)):
            nan_idx = np.where(np.isnan(q_obs))[0]
            print(
                f"âŒ åœºæ¬¡ {event_name} çš„ {FLOOD_EVENT_VARS['OBS_FLOW']} å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}"
            )
            raise ValueError(
                f"Event {event_name} has NaN in {FLOOD_EVENT_VARS['OBS_FLOW']} at index {nan_idx}"
            )
