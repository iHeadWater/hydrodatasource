"""
Author: Wenyu Ouyang
Date: 2025-01-19 18:05:00
LastEditTime: 2025-07-17 17:59:41
LastEditors: Wenyu Ouyang
Description: æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±» - ç»§æ‰¿è‡ªSelfMadeHydroDataset
FilePath: \hydrodatasource\hydrodatasource\reader\floodevent.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import glob
from pathlib import Path
from hydrodatasource.configs.data_consts import FLOOD_EVENT_VARS
from hydrodatasource.utils.utils import streamflow_unit_conv
from hydrodatasource.reader.data_source import SelfMadeHydroDataset


class FloodEventDatasource(SelfMadeHydroDataset):
    """
    æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±»

    ç»§æ‰¿è‡ªSelfMadeHydroDatasetï¼Œä¸“é—¨ç”¨äºå¤„ç†åˆ°é€ä¸ªæ´ªæ°´åœºæ¬¡æ•°æ®ï¼Œ
    åŒ…æ‹¬è¯»å–æµåŸŸé¢ç§¯ã€å•ä½è½¬æ¢ã€åœºæ¬¡æå–ç­‰åŠŸèƒ½ã€‚
    æ”¯æŒè¯»å–ç”Ÿæˆçš„æ´ªæ°´åœºæ¬¡æ•°æ®å¹¶æ‹¼æ¥çœŸå®æ•°æ®ã€‚
    """

    def __init__(
        self,
        data_path: str,
        dataset_name: str = "songliaorrevents",
        time_unit: Optional[List[str]] = None,
        flow_unit: str = "mm/3h",
        augmented_data_path: Optional[str] = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–æµåŸŸåœºæ¬¡æ•°æ®é›†

        Args:
            data_path: æ•°æ®è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
            time_unit: æ—¶é—´å•ä½åˆ—è¡¨ï¼Œé»˜è®¤ä¸º["3h"]
            flow_unit: å¾„æµå•ä½ï¼Œé»˜è®¤ä¸º"mm/3h"
            augmented_data_path: ç”Ÿæˆæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¯é€‰
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        if time_unit is None:
            time_unit = ["3h"]
        # sometimes we load the data with different flow unit
        # so we need to store the flow unit
        self.flow_unit = flow_unit
        self.augmented_data_path = augmented_data_path
        super().__init__(
            data_path=data_path,
            download=False,
            time_unit=time_unit,
            dataset_name=dataset_name,
            **kwargs,
        )

    def _get_ts_file_prefix_(self, dataset_name: str, version: str = None) -> str:
        """
        é‡å†™æ—¶åºæ–‡ä»¶å‰ç¼€æ–¹æ³•ï¼Œä¸ºç”Ÿæˆæ•°æ®æ·»åŠ æ ‡è¯†

        Args:
            dataset_name: æ•°æ®é›†åç§°
            version: ç‰ˆæœ¬å·

        Returns:
            å¸¦æœ‰augmentedæ ‡è¯†çš„æ–‡ä»¶å‰ç¼€
        """
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºç¡€å‰ç¼€
        base_prefix = super()._get_ts_file_prefix_(dataset_name, version)

        if self.augmented_data_path:
            return f"augmented_{base_prefix}" if base_prefix else "augmented_"
        return base_prefix

    def set_data_description(self):
        """è®¾ç½®æ•°æ®æºæè¿°ï¼Œç»§æ‰¿çˆ¶ç±»æ–¹æ³•å¹¶æ‰©å±•"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().set_data_description()

        # å¦‚æœæœ‰ç”Ÿæˆæ•°æ®è·¯å¾„ï¼Œæ·»åŠ ç›¸å…³æè¿°
        if self.augmented_data_path:
            prefix = "augmented_" if self.augmented_data_path else ""
            self.data_source_description.update(
                {
                    "data_type": f"{prefix}flood_events",
                    "augmented_data_path": self.augmented_data_path,
                    "supports_augmented_data": True,
                }
            )

    def extract_flood_events(
        self, df: pd.DataFrame
    ) -> List[Tuple[np.ndarray, np.ndarray, str]]:
        """
        ä»æ•°æ®æ¡†ä¸­æå–æ´ªæ°´äº‹ä»¶ï¼Œè¿”å›å‡€é›¨ã€å¾„æµæ•°ç»„å’Œæ´ªå³°æ—¥æœŸ

        Args:
            df: ç«™ç‚¹æ•°æ®æ¡†

        Returns:
            List[Tuple[np.ndarray, np.ndarray, str]]:
                (å‡€é›¨æ•°ç»„, å¾„æµæ•°ç»„, æ´ªå³°æ—¥æœŸ) åˆ—è¡¨
        """
        events: List[Tuple[np.ndarray, np.ndarray, str]] = []
        # æ‰¾åˆ°è¿ç»­çš„flood_event > 0åŒºé—´
        flood_mask = df["flood_event"] > 0

        if not flood_mask.any():
            return events

        # æ‰¾è¿ç»­åŒºé—´
        in_event = False
        start_idx = None

        for idx, is_flood in enumerate(flood_mask):
            if is_flood and not in_event:
                start_idx = idx
                in_event = True
            elif not is_flood and in_event:
                # äº‹ä»¶ç»“æŸï¼Œæå–æ•°æ®
                event_data = df.iloc[start_idx:idx]
                net_rain = event_data["net_rain"].values
                inflow = event_data["inflow"].values
                event_times = event_data["time"].values

                # åŸºæœ¬éªŒè¯
                if len(net_rain) > 0 and len(inflow) > 0 and np.nansum(inflow) > 1e-6:
                    # è·å–åœºæ¬¡å¼€å§‹å’Œç»“æŸæ—¶é—´
                    start_time = event_times[0]
                    end_time = event_times[-1]

                    # è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ (YYYYMMDDHH)
                    def time_to_ten_digits(time_obj):
                        """å°†æ—¶é—´å¯¹è±¡è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ YYYYMMDDHH"""
                        if isinstance(time_obj, np.datetime64):
                            # å¦‚æœæ˜¯numpy datetime64å¯¹è±¡
                            return (
                                time_obj.astype("datetime64[h]")
                                .astype(str)
                                .replace("-", "")
                                .replace("T", "")
                                .replace(":", "")
                            )
                        elif hasattr(time_obj, "strftime"):
                            # å¦‚æœæ˜¯datetimeå¯¹è±¡
                            return time_obj.strftime("%Y%m%d%H")
                        else:
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                            try:
                                if isinstance(time_obj, str):
                                    dt = datetime.fromisoformat(
                                        time_obj.replace("Z", "+00:00")
                                    )
                                    return dt.strftime("%Y%m%d%H")
                                else:
                                    return "0000000000"  # é»˜è®¤å€¼
                            except Exception:
                                return "0000000000"  # é»˜è®¤å€¼

                    start_digits = time_to_ten_digits(start_time)
                    end_digits = time_to_ten_digits(end_time)

                    # ç»„åˆæˆåœºæ¬¡åç§°ï¼šèµ·å§‹æ—¶é—´_ç»“æŸæ—¶é—´
                    event_name = f"{start_digits}_{end_digits}"

                    events.append((net_rain, inflow, event_name))

                in_event = False
        return events

    def parse_augmented_event_metadata(
        self, file_path: str
    ) -> Optional[Dict[str, str]]:
        """
        è§£æç”Ÿæˆäº‹ä»¶æ–‡ä»¶çš„å…ƒä¿¡æ¯

        Args:
            file_path: äº‹ä»¶æ–‡ä»¶è·¯å¾„

        Returns:
            Dict: åŒ…å«æºäº‹ä»¶ã€ç¼©æ”¾å› å­ç­‰å…ƒä¿¡æ¯çš„å­—å…¸
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            metadata = {}
            for line in lines:
                if line.startswith("# "):
                    if "Source:" in line:
                        source_file = line.split("Source:")[1].strip()
                        metadata["source_event"] = source_file.replace(".csv", "")
                    elif "Scale Factor:" in line:
                        metadata["scale_factor"] = line.split("Scale Factor:")[
                            1
                        ].strip()
                    elif "Start Time:" in line:
                        metadata["start_time"] = line.split("Start Time:")[1].strip()
                    elif "End Time:" in line:
                        metadata["end_time"] = line.split("End Time:")[1].strip()
                    elif "Sample ID:" in line:
                        metadata["sample_id"] = line.split("Sample ID:")[1].strip()
                elif not line.startswith("#"):
                    break  # ç»“æŸå…ƒä¿¡æ¯è¯»å–

            return metadata
        except Exception as e:
            print(f"è§£æå…ƒä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return None

    def read_augmented_events(self) -> List[Dict]:
        """
        è¯»å–æ‰€æœ‰ç”Ÿæˆçš„äº‹ä»¶æ•°æ®

        Returns:
            List[Dict]: ç”Ÿæˆäº‹ä»¶æ•°æ®åˆ—è¡¨
        """
        if not self.augmented_data_path:
            return []

        augmented_path = Path(self.augmented_data_path)
        if not augmented_path.exists():
            print(f"ç”Ÿæˆæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.augmented_data_path}")
            return []

        event_files = glob.glob(str(augmented_path / "event_*_aug_*.csv"))

        augmented_events = []
        for file_path in sorted(event_files):
            try:
                # è§£æå…ƒä¿¡æ¯
                metadata = self.parse_augmented_event_metadata(file_path)
                if not metadata:
                    continue

                # è¯»å–æ•°æ®
                df = pd.read_csv(file_path, comment="#")
                if df.empty:
                    continue

                # å¤„ç†æ•°æ®
                event_data = {
                    "file_path": file_path,
                    "metadata": metadata,
                    "data": df,
                    "start_time": metadata.get("start_time"),
                    "end_time": metadata.get("end_time"),
                    "source_event": metadata.get("source_event"),
                }

                augmented_events.append(event_data)

            except Exception as e:
                print(f"è¯»å–ç”Ÿæˆäº‹ä»¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                continue

        return augmented_events

    def get_warmup_data(
        self, station_id: str, source_event_name: str, warmup_hours: int = 240
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡å®šäº‹ä»¶çš„é¢„çƒ­æœŸæ•°æ®

        Args:
            station_id: ç«™ç‚¹ID
            source_event_name: æºäº‹ä»¶åç§° (å¦‚ event_1994081520_1994081805)
            warmup_hours: é¢„çƒ­æœŸå°æ—¶æ•°

        Returns:
            é¢„çƒ­æœŸæ•°æ®DataFrame
        """
        try:
            # ä»äº‹ä»¶åç§°è§£ææ—¶é—´
            parts = source_event_name.replace("event_", "").split("_")
            if len(parts) < 2:
                return None

            start_time_str = parts[0]  # å¦‚ 1994081520
            # è½¬æ¢ä¸ºdatetime
            start_time = datetime.strptime(start_time_str, "%Y%m%d%H")

            # è®¡ç®—é¢„çƒ­æœŸå¼€å§‹æ—¶é—´
            warmup_start = start_time - pd.Timedelta(hours=warmup_hours)

            # ä½¿ç”¨çˆ¶ç±»æ–¹æ³•è¯»å–æ•°æ®
            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=[
                    warmup_start.strftime("%Y-%m-%d %H"),
                    start_time.strftime("%Y-%m-%d %H"),
                ],
                var_lst=["gauge_rain", "streamflow"],
            )["3h"]

            if xr_ds is None:
                return None

            return xr_ds.to_dataframe().loc[station_id].reset_index()
        except Exception as e:
            print(f"è·å–é¢„çƒ­æœŸæ•°æ®å¤±è´¥: {e}")
            return None

    def create_continuous_timeseries(
        self, station_id: str, start_year: int = 1960, end_year: int = 2500
    ) -> Optional[pd.DataFrame]:
        """
        åˆ›å»ºè¿ç»­æ—¶é—´åºåˆ—ï¼Œæ‹¼æ¥çœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®

        Args:
            station_id: ç«™ç‚¹ID
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½

        Returns:
            æ‹¼æ¥åçš„è¿ç»­æ—¶é—´åºåˆ—DataFrame
        """
        try:
            # 1. è¯»å–çœŸå®æ•°æ®ï¼ˆåˆ°å½“å‰æ—¶é—´ï¼‰
            current_year = datetime.now().year
            real_data = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=[f"{start_year}-01-01", f"{current_year}-12-31"],
                var_lst=["gauge_rain", "streamflow"],
            )["3h"]

            if real_data is None:
                return None

            real_df = real_data.to_dataframe().loc[station_id].reset_index()

            # 2. è¯»å–ç”Ÿæˆæ•°æ®
            augmented_events = self.read_augmented_events()
            if not augmented_events:
                return real_df

            # 3. åˆ›å»ºå®Œæ•´æ—¶é—´èŒƒå›´
            full_time_range = pd.date_range(
                start=f"{start_year}-01-01", end=f"{end_year}-12-31 23:00", freq="3H"
            )

            # 4. åˆå§‹åŒ–å®Œæ•´DataFrame
            full_df = pd.DataFrame(
                {
                    "time": full_time_range,
                    "gauge_rain": np.nan,
                    "streamflow": np.nan,
                }
            )

            # 5. å¡«å…¥çœŸå®æ•°æ®
            real_df["time"] = pd.to_datetime(real_df["time"])
            full_df = full_df.set_index("time")
            real_df = real_df.set_index("time")

            # ä½¿ç”¨çœŸå®æ•°æ®æ›´æ–°
            full_df.update(real_df[["gauge_rain", "streamflow"]])

            # 6. å¤„ç†æ¯ä¸ªç”Ÿæˆäº‹ä»¶
            for event in augmented_events:
                event_df = event["data"].copy()
                metadata = event["metadata"]
                source_event = metadata.get("source_event")

                if not source_event:
                    continue

                # è·å–é¢„çƒ­æœŸæ•°æ®
                warmup_df = self.get_warmup_data(station_id, source_event)

                # å¤„ç†ç”Ÿæˆäº‹ä»¶æ—¶é—´
                event_df["time"] = pd.to_datetime(event_df["time"])

                # å˜é‡é‡å‘½å
                if "flow_m3s" in event_df.columns:
                    event_df = event_df.rename(columns={"flow_m3s": "streamflow"})

                # ä»é¢„çƒ­æœŸæ•°æ®è·å–å¯¹åº”çš„é™é›¨æ•°æ®
                if warmup_df is not None and "gauge_rain" in warmup_df.columns:
                    # å°†é¢„çƒ­æœŸé™é›¨æ•°æ®æ˜ å°„åˆ°ç”Ÿæˆäº‹ä»¶æ—¶é—´
                    warmup_rain = warmup_df["gauge_rain"].values
                    if len(warmup_rain) >= len(event_df):
                        # å–é¢„çƒ­æœŸåé¢éƒ¨åˆ†å¯¹åº”äº‹ä»¶æœŸé—´çš„é™é›¨
                        event_df["gauge_rain"] = warmup_rain[-len(event_df) :]
                    else:
                        # å¦‚æœé¢„çƒ­æœŸä¸è¶³ï¼Œç”¨å¯ç”¨æ•°æ®å¡«å……
                        event_df["gauge_rain"] = list(warmup_rain) + [np.nan] * (
                            len(event_df) - len(warmup_rain)
                        )

                # æ›´æ–°åˆ°å®Œæ•´DataFrame
                event_df = event_df.set_index("time")
                full_df.update(event_df[["gauge_rain", "streamflow"]])

            # 7. é‡ç½®ç´¢å¼•å¹¶è¿”å›
            full_df = full_df.reset_index()
            return full_df

        except Exception as e:
            print(f"åˆ›å»ºè¿ç»­æ—¶é—´åºåˆ—å¤±è´¥: {e}")
            return None

    def read_ts_xrdataset(
        self,
        gage_id_lst: list = None,
        t_range: list = None,
        var_lst: list = None,
        **kwargs,
    ) -> dict:
        """
        é‡è½½è¯»å–æ—¶é—´åºåˆ—æ•°æ®æ–¹æ³•ï¼Œæ”¯æŒç”Ÿæˆæ•°æ®

        Args:
            gage_id_lst: ç«™ç‚¹IDåˆ—è¡¨
            t_range: æ—¶é—´èŒƒå›´
            var_lst: å˜é‡åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            xarrayæ•°æ®é›†æˆ–æ‹¼æ¥åçš„æ•°æ®
        """
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆæ•°æ®è·¯å¾„ï¼Œä½¿ç”¨çˆ¶ç±»æ–¹æ³•
        if not self.augmented_data_path:
            return super().read_ts_xrdataset(gage_id_lst, t_range, var_lst, **kwargs)

        # æ£€æŸ¥æ—¶é—´èŒƒå›´æ˜¯å¦æ¶‰åŠæœªæ¥æ•°æ®
        start_time = pd.to_datetime(t_range[0])
        end_time = pd.to_datetime(t_range[1])
        current_time = pd.to_datetime(datetime.now())

        # å¦‚æœå®Œå…¨æ˜¯å†å²æ•°æ®ï¼Œä½¿ç”¨çˆ¶ç±»æ–¹æ³•
        if end_time <= current_time:
            return super().read_ts_xrdataset(gage_id_lst, t_range, var_lst, **kwargs)

        # å¦‚æœæ¶‰åŠæœªæ¥æ•°æ®ï¼Œä½¿ç”¨æ‹¼æ¥æ–¹æ³•
        result_dict = {}
        for station_id in gage_id_lst:
            continuous_df = self.create_continuous_timeseries(
                station_id, start_year=start_time.year, end_year=end_time.year
            )

            if continuous_df is not None:
                # ç­›é€‰æ—¶é—´èŒƒå›´
                mask = (continuous_df["time"] >= start_time) & (
                    continuous_df["time"] <= end_time
                )
                filtered_df = continuous_df[mask]

                # è½¬æ¢ä¸ºxarrayæ ¼å¼
                import xarray as xr

                ds = xr.Dataset.from_dataframe(filtered_df.set_index(["time"]))
                ds = ds.expand_dims(dim={"basin": [station_id]})
                result_dict["3h"] = ds

        return result_dict if result_dict else None

    def create_event_dict(
        self,
        net_rain: np.ndarray,
        inflow: np.ndarray,
        event_name: str,
        include_peak_obs: bool = True,
    ) -> Optional[Dict]:
        """
        å°†å‡€é›¨å’Œå¾„æµæ•°ç»„è½¬æ¢ä¸ºæ ‡å‡†äº‹ä»¶å­—å…¸æ ¼å¼

        Parameters
        ----------
        net_rain: np.ndarray
            å‡€é›¨æ•°ç»„
        inflow: np.ndarray
            å¾„æµæ•°ç»„
        event_name: str
            æ´ªå³°æ—¥æœŸï¼ˆ8ä½æ•°å­—æ ¼å¼ï¼‰
        include_peak_obs: bool
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼

        Returns
        -------
            Dict: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸ï¼Œä¸uh_utils.pyå®Œå…¨å…¼å®¹
        """
        try:
            # è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            valid_rain_mask = ~np.isnan(net_rain) & (net_rain > 0)
            m_eff = np.sum(valid_rain_mask)

            if m_eff == 0:
                return None

            # éªŒè¯å¾„æµæ•°æ®
            if np.nansum(inflow) < 1e-6:
                return None

            # åˆ›å»ºæ ‡å‡†æ ¼å¼å­—å…¸ï¼ˆä¸uh_utils.pyæœŸæœ›çš„keyå®Œå…¨ä¸€è‡´ï¼‰
            event_dict = {
                FLOOD_EVENT_VARS["NET_RAIN"]: net_rain,  # æœ‰æ•ˆé™é›¨ï¼ˆå‡€é›¨ï¼‰
                FLOOD_EVENT_VARS["OBS_FLOW"]: inflow,  # è§‚æµ‹å¾„æµ
                "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
                "n_specific": len(net_rain),  # å•ä½çº¿é•¿åº¦
                # æ·»åŠ filepathå­—æ®µé¿å…KeyError
                "filepath": f"event_{event_name}.csv",
            }

            # æ·»åŠ æ´ªå³°è§‚æµ‹å€¼
            if include_peak_obs:
                peak_flow = np.nanmax(inflow)
                if peak_flow < 1e-6:
                    return None
                event_dict["peak_obs"] = peak_flow

            return event_dict

        except Exception:
            return None

    def _load_1basin_flood_events(
        self,
        station_id: Optional[str] = None,
        include_peak_obs: bool = True,
        verbose: bool = True,
    ) -> Optional[List[Dict]]:
        """
        åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®

        Parameters
        ----------
        station_id:
            æŒ‡å®šç«™ç‚¹IDï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç«™ç‚¹
        include_peak_obs:
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose:
            æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns
        -------
            List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰ç®—æ³•å®Œå…¨å…¼å®¹
        """
        # è·å–æµåŸŸé¢ç§¯
        basin_area_km2 = None
        if station_id:
            try:
                basin_area_km2 = self.read_area([station_id])
                if verbose:
                    print(f"ğŸ“Š è¯»å–åˆ°æµåŸŸé¢ç§¯: {basin_area_km2} kmÂ²")
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ æ— æ³•è¯»å–æµåŸŸé¢ç§¯: {str(e)}")

        try:
            if verbose:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®...")
                if station_id:
                    print(f"   æŒ‡å®šç«™ç‚¹: {station_id}")

            all_events = []
            total_events = 0

            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=["1960-01-01", "2024-12-31"],
                var_lst=["inflow", "net_rain", "flood_event"],
                # recache=True,
            )["3h"]
            if self.flow_unit == "mm/3h":
                xr_ds["inflow"] = streamflow_unit_conv(
                    xr_ds[["inflow"]], basin_area_km2, target_unit="mm/3h"
                )["inflow"]
            elif self.flow_unit == "m^3/s":
                pass
            else:
                raise ValueError(f"Unsupported flow unit: {self.flow_unit}")
            df = xr_ds.to_dataframe()
            if df is None:
                return None

            # æå–æ´ªæ°´äº‹ä»¶
            flood_events = self.extract_flood_events(df.loc[station_id].reset_index())

            if not flood_events:
                if verbose:
                    print(f"  âš ï¸  {station_id}: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ´ªæ°´äº‹ä»¶")
                return None

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            station_event_count = 0
            for net_rain, inflow, event_name in flood_events:
                event_dict = self.create_event_dict(
                    net_rain, inflow, event_name, include_peak_obs
                )
                if event_dict is not None:
                    all_events.append(event_dict)
                    station_event_count += 1

            if verbose and station_event_count > 0:
                print(f"  âœ… {station_id}: æˆåŠŸå¤„ç† {station_event_count} ä¸ªæ´ªæ°´äº‹ä»¶")
                total_events += station_event_count

            if not all_events:
                if verbose:
                    print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ´ªæ°´äº‹ä»¶æ•°æ®")
                return None

            if verbose:
                print(f"âœ… æ€»å…±æˆåŠŸåŠ è½½ {len(all_events)} ä¸ªæ´ªæ°´äº‹ä»¶")

            return all_events

        except Exception as e:
            if verbose:
                print(f"âŒ åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None


def _calculate_event_characteristics(
    event: Dict, delta_t_hours: float = 3.0
) -> Optional[Dict]:
    """
    è®¡ç®—æ´ªæ°´äº‹ä»¶çš„è¯¦ç»†ç‰¹å¾æŒ‡æ ‡ï¼Œç”¨äºç”»å›¾å’Œåˆ†æ

    Parameters
    ----------
        event: dict
            äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« 'P_eff' (å‡€é›¨) å’Œ 'Q_obs_eff' (å¾„æµ) æ•°ç»„
        delta_t_hours: float
            æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns
    -------
        Dict: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡

    Calculated metrics:
        - peak_obs: æ´ªå³°æµé‡ (mÂ³/s)
        - runoff_volume_m3: æ´ªé‡ (mÂ³)
        - runoff_duration_hours: æ´ªæ°´å†æ—¶ (å°æ—¶)
        - total_net_rain: æ€»å‡€é›¨é‡ (mm)
        - lag_time_hours: æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
    """
    try:
        # æå–æ•°æ®
        net_rain = event.get(FLOOD_EVENT_VARS["NET_RAIN"], [])
        direct_runoff = event.get(FLOOD_EVENT_VARS["OBS_FLOW"], [])

        net_rain = np.array(net_rain)
        direct_runoff = np.array(direct_runoff)

        # è½¬æ¢ä¸ºç§’
        delta_t_seconds = delta_t_hours * 3600.0

        # 1. è®¡ç®—æ´ªå³°æµé‡
        peak_obs = np.max(direct_runoff)
        if peak_obs < 1e-6:
            return None

        # 2. è®¡ç®—æ´ªé‡ (mÂ³)
        runoff_volume_m3 = np.sum(direct_runoff) * delta_t_seconds

        # 3. è®¡ç®—æ´ªæ°´å†æ—¶ (å°æ—¶)
        runoff_indices = np.where(direct_runoff > 1e-6)[0]
        if len(runoff_indices) < 2:
            return None
        runoff_duration_hours = (
            runoff_indices[-1] - runoff_indices[0] + 1
        ) * delta_t_hours

        # 4. è®¡ç®—æ€»å‡€é›¨é‡ (mm)
        total_net_rain = np.sum(net_rain)

        # 5. è®¡ç®—æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
        t_peak_flow_idx = np.argmax(direct_runoff)
        t_peak_rain_idx = np.argmax(net_rain)
        lag_time_hours = (t_peak_flow_idx - t_peak_rain_idx) * delta_t_hours

        # 6. è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
        m_eff = len(net_rain)

        # 7. è®¡ç®—å¾„æµæ—¶æ®µæ•°
        n_obs = len(direct_runoff)

        # 8. è®¡ç®—å•ä½çº¿é•¿åº¦
        n_specific = n_obs - m_eff + 1

        return {
            "peak_obs": peak_obs,  # æ´ªå³°æµé‡ (mÂ³/s)
            "runoff_volume_m3": runoff_volume_m3,  # æ´ªé‡ (mÂ³)
            "runoff_duration_hours": runoff_duration_hours,  # æ´ªæ°´å†æ—¶ (å°æ—¶)
            "total_net_rain": total_net_rain,  # æ€»å‡€é›¨é‡ (mm)
            "lag_time_hours": lag_time_hours,  # æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
            "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            "n_obs": n_obs,  # å¾„æµæ—¶æ®µæ•°
            "n_specific": n_specific,  # å•ä½çº¿é•¿åº¦
            "delta_t_hours": delta_t_hours,  # æ—¶æ®µé•¿åº¦
        }
    except Exception as e:
        print(f"è®¡ç®—äº‹ä»¶ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return None


def calculate_events_characteristics(
    events: List[Dict], delta_t_hours: float = 3.0
) -> List[Dict]:
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªæ´ªæ°´äº‹ä»¶çš„ç‰¹å¾æŒ‡æ ‡

    Args:
        events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« 'P_eff' å’Œ 'Q_obs_eff' æ•°ç»„
        delta_t_hours: æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns:
        List[Dict]: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡çš„äº‹ä»¶åˆ—è¡¨
    """
    enhanced_events = []

    for i, event in enumerate(events):
        # è®¡ç®—ç‰¹å¾æŒ‡æ ‡
        characteristics = _calculate_event_characteristics(event, delta_t_hours)

        if characteristics is not None:
            # å°†ç‰¹å¾æŒ‡æ ‡æ·»åŠ åˆ°åŸäº‹ä»¶å­—å…¸ä¸­
            enhanced_event = event.copy()
            enhanced_event.update(characteristics)
            enhanced_events.append(enhanced_event)
        else:
            print(f"âš ï¸ äº‹ä»¶ {i+1} ç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡")

    return enhanced_events


def load_and_preprocess_events_unified(
    data_dir: str,
    station_id: Optional[str] = None,
    include_peak_obs: bool = True,
    verbose: bool = True,
    flow_unit: str = "mm/3h",
    augmented_data_path: Optional[str] = None,
) -> Optional[List[Dict]]:
    """
    å‘åå…¼å®¹çš„ç»Ÿä¸€æ¥å£å‡½æ•°

    Args:
        data_dir: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        station_id: æµåŸŸç«™ç‚¹IDï¼ˆå¯é€‰ï¼‰
        include_peak_obs: æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        flow_unit: æµé‡å•ä½
        augmented_data_path: ç”Ÿæˆæ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰å•ä½çº¿ç®—æ³•å®Œå…¨å…¼å®¹
    """
    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    dataset = FloodEventDatasource(
        data_dir,
        flow_unit=flow_unit,
        augmented_data_path=augmented_data_path,
        trange4cache=["1960-01-01 02", "2024-12-31 23"],
    )
    return dataset._load_1basin_flood_events(station_id, include_peak_obs, verbose)


def check_event_data_nan(all_event_data: List[Dict]):
    """
    æ£€æŸ¥æ‰€æœ‰æ´ªæ°´äº‹ä»¶æ•°æ®ä¸­çš„é™é›¨å’Œå¾„æµæ˜¯å¦æœ‰ç©ºå€¼ï¼Œè‹¥æœ‰åˆ™æŠ¥é”™å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯ã€‚
    Args:
        all_event_data: äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå­—å…¸åŒ…å«P_effã€Q_obs_effã€filepathç­‰ï¼‰
    Raises:
        ValueError: å¦‚æœå‘ç°ç©ºå€¼ï¼ŒæŠ›å‡ºå¼‚å¸¸å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    for event in all_event_data:
        event_name = event.get("filepath", "unknown")
        p_eff = event.get(FLOOD_EVENT_VARS["NET_RAIN"])
        q_obs = event.get(FLOOD_EVENT_VARS["OBS_FLOW"])
        # æ£€æŸ¥é™é›¨
        if p_eff is not None and np.any(np.isnan(p_eff)):
            nan_idx = np.where(np.isnan(p_eff))[0]
            print(f"âŒ åœºæ¬¡ {event_name} çš„ P_eff å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}")
            raise ValueError(f"Event {event_name} has NaN in P_eff at index {nan_idx}")
        # æ£€æŸ¥å¾„æµ
        if q_obs is not None and np.any(np.isnan(q_obs)):
            nan_idx = np.where(np.isnan(q_obs))[0]
            print(
                f"âŒ åœºæ¬¡ {event_name} çš„ "
                f"{FLOOD_EVENT_VARS['OBS_FLOW']} å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}"
            )
            raise ValueError(
                f"Event {event_name} has NaN in "
                f"{FLOOD_EVENT_VARS['OBS_FLOW']} at index {nan_idx}"
            )


# ä½¿ç”¨ç¤ºä¾‹
"""
å®Œå–„åçš„FloodEventDatasourceç±»ä½¿ç”¨ç¤ºä¾‹ï¼š

# 1. åŸºæœ¬ç”¨æ³• - ä»…è¯»å–çœŸå®å†å²æ•°æ®
dataset = FloodEventDatasource(
    data_path="/path/to/historical/data",
    dataset_name="songliaorrevents",
    flow_unit="mm/3h"
)

# 2. é«˜çº§ç”¨æ³• - æ”¯æŒç”Ÿæˆæ•°æ®çš„è¿ç»­æ—¶é—´åºåˆ—
dataset = FloodEventDatasource(
    data_path="/path/to/historical/data",
    dataset_name="songliaorrevents",
    flow_unit="mm/3h",
    augmented_data_path="D:/Code/hydromodel_dev/results/real_data_augmentation_shared"
)

# è¯»å–å†å²å’Œç”Ÿæˆæ•°æ®çš„æ··åˆæ—¶é—´åºåˆ—ï¼ˆå†å²+æœªæ¥ï¼‰
xr_data = dataset.read_ts_xrdataset(
    gage_id_lst=["station_id"],
    t_range=["1960-01-01", "2500-12-31"],  # åŒ…å«æœªæ¥æ—¶é—´
    var_lst=["gauge_rain", "streamflow"]
)

# è¯»å–ç”Ÿæˆçš„äº‹ä»¶æ•°æ®
augmented_events = dataset.read_augmented_events()

# è·å–é¢„çƒ­æœŸæ•°æ®
warmup_data = dataset.get_warmup_data(
    station_id="station_id",
    source_event_name="event_1994081520_1994081805",
    warmup_hours=240
)

# åˆ›å»ºè¿ç»­æ—¶é—´åºåˆ—
continuous_df = dataset.create_continuous_timeseries(
    station_id="station_id",
    start_year=1960,
    end_year=2500
)

# CacheåŠŸèƒ½ - ç”Ÿæˆæ•°æ®ä¼šè‡ªåŠ¨æ·»åŠ "augmented_"å‰ç¼€
dataset.cache_xrdataset()

ä¸»è¦åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. å‘åå…¼å®¹ï¼šä¸ä¼ augmented_data_pathæ—¶ï¼Œè¡Œä¸ºä¸åŸç±»å®Œå…¨ç›¸åŒ
2. è‡ªåŠ¨æ‹¼æ¥ï¼šè¯»å–æœªæ¥æ—¶é—´æ—¶è‡ªåŠ¨æ‹¼æ¥å†å²æ•°æ®å’Œç”Ÿæˆæ•°æ®
3. é¢„çƒ­æœŸæ”¯æŒï¼šè‡ªåŠ¨è·å–ç”Ÿæˆäº‹ä»¶å¯¹åº”çš„å†å²é™é›¨æ•°æ®
4. å˜é‡æ˜ å°„ï¼šflow_m3s -> streamflow, ä»åŸæ•°æ®è·å–gauge_rain
5. Cacheæ”¯æŒï¼šç”Ÿæˆæ•°æ®cacheæ–‡ä»¶è‡ªåŠ¨æ·»åŠ "augmented_"å‰ç¼€
6. å…ƒä¿¡æ¯è§£æï¼šè‡ªåŠ¨è§£æç”Ÿæˆäº‹ä»¶æ–‡ä»¶çš„Sourceã€Scale Factorç­‰ä¿¡æ¯
7. æ—¶é—´æ˜ å°„ï¼šç”Ÿæˆæ•°æ®çš„å‡æ—¶é—´è‡ªåŠ¨æ˜ å°„åˆ°è¿ç»­æ—¶é—´åºåˆ—ä¸­
"""
