{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hydrodatasource","text":"<ul> <li>Free software: BSD license</li> <li>Documentation: https://iHeadWater.github.io/hydrodatasource</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>While libraries like hydrodataset exist for accessing standardized, public hydrological datasets (e.g., CAMELS), a common challenge is working with data that isn't in a ready-to-use format. This includes non-public industry data, data from local authorities, or custom datasets compiled for specific research projects.</p> <p><code>hydrodatasource</code> is designed to solve this problem. It provides a flexible framework to read, process, and clean these custom datasets, preparing them for hydrological modeling and analysis.</p> <p>The core of this framework is the <code>SelfMadeHydroDataset</code> class, which allows you to easily access your own data by organizing it into a simple, predefined directory structure.</p>"},{"location":"#reading-custom-datasets-with-selfmadehydrodataset","title":"Reading Custom Datasets with <code>SelfMadeHydroDataset</code>","text":"<p>This is the primary use case for <code>hydrodatasource</code>. If you have your own basin-level time series and attribute data, you can use this class to load it seamlessly.</p>"},{"location":"#1-prepare-your-data-directory","title":"1. Prepare Your Data Directory","text":"<p>First, organize your data into the following folder structure:</p> <pre><code>/path/to/your_data_root/\n    \u2514\u2500\u2500 my_custom_dataset/              # Your dataset's name\n        \u251c\u2500\u2500 attributes/\n        \u2502   \u2514\u2500\u2500 attributes.csv\n        \u251c\u2500\u2500 shapes/\n        \u2502   \u2514\u2500\u2500 basins.shp\n        \u2514\u2500\u2500 timeseries/\n            \u251c\u2500\u2500 1D/                     # Sub-folder for each time resolution (e.g., daily)\n            \u2502   \u251c\u2500\u2500 basin_01.csv\n            \u2502   \u251c\u2500\u2500 basin_02.csv\n            \u2502   \u2514\u2500\u2500 ...\n            \u2514\u2500\u2500 1D_units_info.json      # JSON file with unit information\n</code></pre> <ul> <li><code>attributes/attributes.csv</code>: A CSV file containing static basin attributes (e.g., area, mean elevation). Must include a <code>basin_id</code> column that matches the filenames in the <code>timeseries</code> folder.</li> <li><code>shapes/basins.shp</code>: A shapefile with the polygon geometry for each basin.</li> <li><code>timeseries/1D/</code>: A folder for each time resolution (e.g., <code>1D</code> for daily, <code>3h</code> for 3-hourly). Inside, each CSV file should contain the time series data for a single basin and be named after its <code>basin_id</code>.</li> <li><code>timeseries/1D_units_info.json</code>: A JSON file defining the units for each variable in your time series CSVs (e.g., <code>{\"precipitation\": \"mm/d\", \"streamflow\": \"m3/s\"}</code>).</li> </ul>"},{"location":"#2-read-the-data-in-python","title":"2. Read the Data in Python","text":"<p>Once your data is organized, you can use <code>SelfMadeHydroDataset</code> to read it with just a few lines of code.</p> <pre><code>from hydrodatasource.reader.data_source import SelfMadeHydroDataset\n\n# 1. Define the path to your data's parent directory and the dataset name\ndata_path = \"/path/to/your_data_root/\"\ndataset_name = \"my_custom_dataset\"\n\n# 2. Initialize the reader\n# Specify the time units you want to work with\nreader = SelfMadeHydroDataset(data_path=data_path, dataset_name=dataset_name, time_unit=[\"1D\"])\n\n# 3. Get a list of all available basin IDs\nbasin_ids = reader.read_object_ids()\n\n# 4. Define the time range and variables you want to load\nt_range = [\"2000-01-01\", \"2010-12-31\"]\nvariables_to_read = [\"precipitation\", \"streamflow\", \"temperature\"]\n\n# 5. Read the time series data\n# The result is a dictionary of xarray.Datasets, keyed by time unit\ntimeseries_data = reader.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=t_range,\n    var_lst=variables_to_read,\n    time_units=[\"1D\"]\n)\n\ndaily_data = timeseries_data[\"1D\"]\n\nprint(\"Successfully loaded data:\")\nprint(daily_data)\n\n# You can also read the static attributes\nattributes_data = reader.read_attr_xrdataset(gage_id_lst=basin_ids, var_lst=[\"area\", \"mean_elevation\"])\nprint(\"\\nAttributes:\")\nprint(attributes_data)\n</code></pre>"},{"location":"#other-features","title":"Other Features","text":"<p>Beyond reading data, <code>hydrodatasource</code> also includes modules for:</p> <ul> <li><code>processor</code>: Perform advanced calculations like identifying rainfall-runoff events (<code>dmca_esr.py</code>) and calculating basin-wide mean rainfall from station data (<code>basin_mean_rainfall.py</code>).</li> <li><code>cleaner</code>: Clean raw time series data. This includes tools for smoothing noisy streamflow data, correcting anomalies in rainfall and water level records, and back-calculating reservoir inflow.</li> </ul> <p>The usage of these modules is described in the API Reference. We will add more examples in the future.</p>"},{"location":"#installation","title":"Installation","text":"<p>For standard use, install the package from PyPI:</p> <pre><code>pip install hydrodatasource\n</code></pre>"},{"location":"#development-setup","title":"Development Setup","text":"<p>For developers, it is recommended to use <code>uv</code> to manage the environment, as this project has local dependencies (e.g., <code>hydroutils</code>, <code>hydrodataset</code>).</p> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/iHeadWater/hydrodatasource.git\ncd hydrodatasource\n</code></pre></p> </li> <li> <p>Sync the environment with <code>uv</code>:     This command will install all dependencies, including the local editable packages.     <pre><code>uv sync --all-extras\n</code></pre></p> </li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>This page provides an auto-generated API reference for the key components of the <code>hydrodatasource</code> library.</p>"},{"location":"api/#reader","title":"Reader","text":""},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset","title":"<code>hydrodatasource.reader.data_source.SelfMadeHydroDataset</code>","text":"<p>               Bases: <code>HydroData</code></p> <p>A class for reading hydrodataset, but not really ready-datasets, just some data directorys organized like a HydroDataset.</p> <p>NOTE: We compile forcing data and attr data into a directory, organized like a ready dataset -- like Caravan. Only two directories are needed: attributes and timeseries</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.__init__","title":"<code>__init__(data_path, dataset_name, time_unit=None, **kwargs)</code>","text":"<p>Initialize a self-made Caravan-style dataset.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.__init__--parameters","title":"Parameters","text":"<p>data_path : str     The path to the custom-made data sources' parent directory. dataset_name : str     SelfMadeHydroDataset's name, for example, googleflood or fdsources,     different dataset may use this same datasource class, but they have different dataset_name. time_unit : list, optional     we have different time units, by default None kwargs : dict, optional     additional keyword arguments, by default None</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.cache_attributes_xrdataset","title":"<code>cache_attributes_xrdataset()</code>","text":"<p>Convert all the attributes to a single dataset</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.cache_attributes_xrdataset--returns","title":"Returns","text":"<p>None</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.cache_timeseries_xrdataset","title":"<code>cache_timeseries_xrdataset(**kwargs)</code>","text":"<p>Save all timeseries data in separate NetCDF files for each time unit.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.cache_timeseries_xrdataset--parameters","title":"Parameters","text":"<p>t_range : list, optional     Time range for the data, by default [\"1980-01-01\", \"2023-12-31\"] kwargs : dict, optional     batchsize -- Number of basins to process per batch, by default 100     time_units -- List of time units to process, by default None     start0101_freq -- for freq setting, if the start date is 01-01, set True, by default False     offset_to_utc -- whether to offset the time to UTC, by default False     start_hour_in_a_day -- the start hour in a day (0-23), by default 2 which means 2-5-8-11-14-17-20-23 UTC.                            Chinese basins data always use 08:00 with Beijing Time, so we set the default value to 2.                            Only applicable for sub-daily intervals (currently only \"3h\" is supported)</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.cache_xrdataset","title":"<code>cache_xrdataset(t_range=None, time_units=None)</code>","text":"<p>Save all data in a netcdf file in the cache directory</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.get_attributes_cols","title":"<code>get_attributes_cols()</code>","text":"<p>the constant cols in this data_source</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.get_timeseries_cols","title":"<code>get_timeseries_cols()</code>","text":"<p>the relevant cols in this data_source</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_area","title":"<code>read_area(gage_id_lst=None)</code>","text":"<p>read area of each basin/unit</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_attributes","title":"<code>read_attributes(object_ids=None, constant_cols=None, **kwargs)</code>","text":"<p>2d data (site_num * var_num), non-time-series data</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst=None, unit='mm/d')</code>","text":"<p>read mean precipitation of each basin default unit is mm/d, but one can chose other units and we will convert the unit to the specified unit</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_mean_prcp--parameters","title":"Parameters","text":"<p>gage_id_lst : list, optional     the list of gage ids, by default None unit : str, optional     the unit of precipitation, by default \"mm/d\"</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_mean_prcp--returns","title":"Returns","text":"<p>xr.Dataset     the mean precipitation of each basin</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_timeseries","title":"<code>read_timeseries(object_ids=None, t_range_list=None, relevant_cols=None, **kwargs)</code>","text":"<p>Returns a dictionary containing data with different time scales.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_timeseries--parameters","title":"Parameters","text":"<p>object_ids : list, optional     List of object IDs. Defaults to None. t_range_list : list, optional     List of time ranges. Defaults to None. relevant_cols : list, optional     List of relevant columns. Defaults to None. **kwargs : dict, optional     Additional keyword arguments.     time_units : list, optional         List of time units to process     start0101_freq : bool, optional         For freq setting, if the start date is 01-01, set True     offset_to_utc : bool, optional         Whether to offset the time to UTC     start_hour_in_a_day : int, optional         The start hour in a day for sub-daily intervals (0-23). Default is 2.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_timeseries--returns","title":"Returns","text":"<p>dict     A dictionary containing data with different time scales.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_ts_xrdataset","title":"<code>read_ts_xrdataset(gage_id_lst=None, t_range=None, var_lst=None, **kwargs)</code>","text":"<p>Read time-series xarray dataset from multiple NetCDF files and organize them by time units.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_ts_xrdataset--parameters","title":"Parameters:","text":"<p>gage_id_lst: list     List of gage IDs to select. t_range: list     List of two elements [start_time, end_time] to select time range. var_lst: list     List of variables to select. **kwargs     Additional arguments.</p>"},{"location":"api/#hydrodatasource.reader.data_source.SelfMadeHydroDataset.read_ts_xrdataset--returns","title":"Returns:","text":"<p>dict: A dictionary where each key is a time unit and each value is an xarray.Dataset containing the selected gage IDs, time range, and variables.</p>"},{"location":"api/#processor","title":"Processor","text":""},{"location":"api/#basin-mean-rainfall","title":"Basin Mean Rainfall","text":""},{"location":"api/#hydrodatasource.processor.basin_mean_rainfall.basin_mean_func","title":"<code>hydrodatasource.processor.basin_mean_rainfall.basin_mean_func(df, weights_dict=None)</code>","text":"<p>Generic basin averaging method that supports both arithmetic mean and weighted mean (e.g. Thiessen polygon weights)</p> <p>When some columns have missing values in a row, the function automatically switches to arithmetic mean for that row instead of using weights. This ensures robustness when dealing with incomplete data.</p>"},{"location":"api/#hydrodatasource.processor.basin_mean_rainfall.basin_mean_func--parameters","title":"Parameters","text":"<p>df : DataFrame     Time series DataFrame for multiple stations, with station names as column names;     each column should be a time series of rainfall data for a specific station weights_dict : dict, optional     Dictionary with tuple of station names as keys and list of weights as values.     If None, arithmetic mean is used.</p> the keys of list must be in the same order as the columns of df. <p>hence, an easy way is you give your df with a sorted column names and then use the same order to create the keys of weights_dict. for example: weights_dict = {     (\"st1\", \"st2\", \"st3\", \"st4\"): [0.25, 0.5, 0.1, 0.15], } df = df[[\"st1\", \"st2\", \"st3\", \"st4\"]] then the keys of weights_dict must be in the same order as the columns of df.</p> NOTE <p>we set the format of weights_dict like this because we want to extend it to match the weights_dict key based on the missing data situation and the key in weights_dict. This is a TODO item. if the key in weights_dict matches the columns of df, we use the weights in weights_dict; if the key in weights_dict does not match the columns of df, we use the arithmetic mean. For example, if the columns of df are [\"st1\", \"st2\", \"st3\", \"st4\"], and the weights_dict is: weights_dict = {     (\"st1\", \"st2\", \"st3\", \"st4\"): [0.25, 0.5, 0.1, 0.15],     (\"st1\", \"st2\", \"st3\"): [0.25, 0.5, 0.1],     (\"st3\", \"st4\"): [0.1, 0.15], } then when st4 has missing data, we use the weights in (\"st1\", \"st2\", \"st3\") to calculate the weighted mean; and when st1 and st2 have missing data, we use the weights in (\"st3\", \"st4\") to calculate the weighted mean. Otherwise, we use the arithmetic mean.</p> <p>But this function is not finished yet, and the weights_dict now only supports the case that the keys of weights_dict has all the columns of df; if any column in df is missing, the function will use the arithmetic mean.</p>"},{"location":"api/#hydrodatasource.processor.basin_mean_rainfall.basin_mean_func--returns","title":"Returns","text":"<p>Series     Basin-averaged time series</p>"},{"location":"api/#rainfall-runoff-event-identification","title":"Rainfall-Runoff Event Identification","text":""},{"location":"api/#hydrodatasource.processor.dmca_esr.get_rr_events","title":"<code>hydrodatasource.processor.dmca_esr.get_rr_events(rain, flow, basin_area, max_window=100, max_flow_min=None)</code>","text":"<p>use DMCA-ESR method to identify rainfall-runoff events</p>"},{"location":"api/#hydrodatasource.processor.dmca_esr.get_rr_events--parameters","title":"Parameters","text":"<p>rain : xr.DataArray     the rainfall data flow : xr.DataArray     the streamflow data basin_area : xr.Dataset     a dataset with a variable named area and for each basin max_window: int     number of time intervals for find events; default 100 (for hourly) max_flow_min: list     the minimum of max flow for each basin     value below this will not be considered to look for an event;     default 100 m^3/s Returns</p> <p>dict     the rainfall-runoff events for each basin</p>"},{"location":"api/#hydrodatasource.processor.dmca_esr.get_rr_events--raises","title":"Raises","text":"<p>ValueError     Invalid unit format ValueError     Unsupported unit</p>"},{"location":"api/#cleaner","title":"Cleaner","text":""},{"location":"api/#rainfallcleaner","title":"RainfallCleaner","text":""},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner","title":"<code>hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner</code>","text":"<p>               Bases: <code>Cleaner</code></p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.__init__","title":"<code>__init__(data_folder, output_folder)</code>","text":"<p>All files to be cleaned are in the data_dir</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.__init__--parameters","title":"Parameters","text":"<p>data_dir : type description</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_hourly_extreme","title":"<code>data_check_hourly_extreme(basin_id, climate_extreme_value=None, modify=False)</code>","text":"<p>Check if the daily precipitation values at chosen stations are within a reasonable range. Values larger than the climate extreme value are treated as anomalies. If no climate_extreme_value is provided, the maximum value in the data is used.</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_hourly_extreme--parameters","title":"Parameters","text":"<p>climate_extreme_value : float, optional     Climate extreme threshold for the region, calculated as 95% of the maximum observed DRP.     If not provided, will be calculated as 95% of the maximum DRP value in the data.</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_hourly_extreme--returns","title":"Returns","text":"<p>df_anomaly_stations_periods : pd.DataFrame     DataFrame of anomalies with columns: 'STCD', 'TM', 'DRP'.</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_time_series","title":"<code>data_check_time_series(basin_id, check_type=None, gradient_limit=None, window_size=None, consistent_value=None, modify=False)</code>","text":"<p>Check daily precipitation values at chosen stations for gradient or time consistency anomalies.</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_time_series--parameters","title":"Parameters","text":"<p>basin_id: str     Basin ID. check_type : str     Type of check to perform: \"gradient\" for gradient check, \"consistency\" for time consistency check. gradient_limit : float, optional     Maximum allowable gradient change in precipitation between consecutive days. Used in \"gradient\" check. Default is 10 mm. window_size : int, optional     Size of the window (in hours) to check for time consistency (used in \"consistency\" check). Default is 24 hours. consistent_value : float, optional     The specific precipitation value to check for consistency (used in \"consistency\" check). Default is 0.1 mm.</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_time_series--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame of detected anomalies with columns: 'STCD', 'TM', 'DRP', 'Issue' (where applicable).</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_yearly","title":"<code>data_check_yearly(basin_id, year_range=None, diff_range=None, min_true_percentage=0.75, min_consecutive_years=3, modify=False)</code>","text":"<p>\u8ba1\u7b97\u9065\u611f\u6570\u636e\u4e0e\u7ad9\u70b9\u6570\u636e\u4e4b\u95f4\u7684\u964d\u6c34\u5dee\u5f02\uff0c\u8bc4\u4f30\u7ad9\u70b9\u53ef\u9760\u6027\uff0c\u5e76\u8fd4\u56de\u53ef\u4fe1\u4efb\u7684\u7ad9\u70b9\u5217\u8868\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_yearly--_1","title":"\u53c2\u6570:","text":"<p>basin_id : str     Basin ID year_range : list, \u53ef\u9009     \u8981\u7b5b\u9009\u7684\u5e74\u4efd\u8303\u56f4\uff0c\u9ed8\u8ba4\u662f [2010, 2024]\u3002 diff_range : list, \u53ef\u9009     \u7ad9\u70b9\u6570\u636e\u548c\u9065\u611f\u6570\u636e\u4e4b\u95f4\u7684ratio\u5dee\u5f02\u8303\u56f4     0.5 means station data is 0.5 times of reanalysis data     2.0 means station data is 2 times of reanalysis data min_true_percentage : float, \u53ef\u9009     \u8981\u6c42\u53ef\u4fe1\u5e74\u4efd\u7684\u6700\u5c0f\u6bd4\u4f8b\uff0c\u9ed8\u8ba4 0.75\u3002 min_consecutive_years : int, \u53ef\u9009     \u6700\u5c0f\u8fde\u7eed\u53ef\u4fe1\u5e74\u4efd\u6570\uff0c\u9ed8\u8ba4 3\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.data_check_yearly--_2","title":"\u8fd4\u56de:","text":"<p>result_df : pd.DataFrame     \u53ef\u4fe1\u7ad9\u70b9\u7684 DataFrame\uff0c\u5305\u542b 'STCD'\u3001'Latitude'\u3001'Longitude' \u548c 'Reason' \u5217\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.rainfall_clean","title":"<code>rainfall_clean(basin_id, **kwargs)</code>","text":"<p>the station gauged rainfall data cleaning pipeline</p>"},{"location":"api/#hydrodatasource.cleaner.rainfall_cleaner.RainfallCleaner.read_and_concat_csv","title":"<code>read_and_concat_csv(basin_id)</code>","text":"<p>\u8bfb\u53d6\u5e76\u5408\u5e76\u6587\u4ef6\u5939\u4e0b\u7684\u6240\u6709 CSV \u6587\u4ef6</p>"},{"location":"api/#reservoirinflowbacktrack","title":"ReservoirInflowBacktrack","text":""},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack","title":"<code>hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack</code>","text":"<p>               Bases: <code>Cleaner</code></p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.__init__","title":"<code>__init__(data_folder, output_folder)</code>","text":"<p>Back-calculating inflow of reservior</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.__init__--parameters","title":"Parameters","text":"<p>data_folder : str     the folder of reservoir data output_folder : type     where we put inflow data</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.back_calculation","title":"<code>back_calculation(rsvr_id, clean_w_path, original_file, output_folder)</code>","text":"<p>Back-calculate inflow from reservoir storage data NOTE: each time has three columns: I Q W -- I is the inflow, Q is the outflow, W is the reservoir storage Generally, in sql database, a time means the end of previous time period For example, a hourly database, 13:00 means 12:00-13:00 period because the data is GOT at 13:00 (we cannot observe future) Hence, for this function, W means the storage at the end of the time period, I and Q means the inflow and outflow of the time period So we need to use W of the previous time as the initial water storage of the time period. Hence, I1 = Q1 + (W1 - W0)</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.back_calculation--parameters","title":"Parameters","text":"<p>rsvr_id : str     The ID of the reservoir data_path : str     the path to the cleaned_w_data file original_file: str     the path to the original file output_folder : str     where to save the back calculated data</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.back_calculation--returns","title":"Returns","text":"<p>str     the path to the result file</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.clean_w","title":"<code>clean_w(rsvr_id, file_path, output_folder, fit_method='quadratic', zw_curve_std_times=3.0, remove_zw_outliers=False)</code>","text":"<p>Remove abnormal reservoir capacity data</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.clean_w--parameters","title":"Parameters","text":"<p>rsvr_id : str     The ID of the reservoir file_path : str     Path to the input file output_folder : str     Path to the output folder fit_method : str, optional     z-w curve fitting method, by default \"quadratic\"     TODO: MORE METHODS need to be supported; power is also need to be debugged zw_curve_std_times: float, optional     the times of standard deviation to remove outliers, by default 3 remove_zw_outliers: bool, optional     whether to remove outliers for z-w curve fitting, by default False</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.clean_w--returns","title":"Returns","text":"<p>str     Path to the cleaned data file</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.delete_negative_inq","title":"<code>delete_negative_inq(rsvr_id, inflow_data_path, original_file, output_folder, negative_deal_window=7, negative_deal_stride=4)</code>","text":"<p>remove negative inflow values with a rolling window the negative value will be adjusted to positvie ones to make the total inflow consistent for example,  1, -1, 1, -1 will be adjusted to 0, 0, 0, 0 so that wate balance is kept but note that as the window has stride, maybe the final few values will not be adjusted</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.delete_negative_inq--parameters","title":"Parameters","text":"<p>rsvr_id : str     the id of the reservoir inflow_data_path : str     the data file after back_calculation original_file : str     the original file output_folder : str     where to save the data negative_deal_window : int, optional     the window to deal with negative values, by default 7 negative_deal_stride : int, optional     the stride of window, by default 4</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.delete_negative_inq--returns","title":"Returns","text":"<p>str     the path to the result file</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.insert_inq","title":"<code>insert_inq(rsvr_id, inflow_data_path, original_file, output_folder)</code>","text":"<p>make inflow data as hourly data as original data is not strictly hourly data and insert inq with linear interpolation</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.insert_inq--parameters","title":"Parameters","text":"<p>rsvr_id : str     the id of the reservoir inflow_data_path : str     the data file after delete negative inflow values original_file : str     the original file output_folder : str     where to save the data</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.insert_inq--returns","title":"Returns","text":"<p>str     the path to the result file</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.rsvr_inflow_clean","title":"<code>rsvr_inflow_clean(**kwargs)</code>","text":"<p>The reservoir inflow data cleaning pipeline</p>"},{"location":"api/#hydrodatasource.cleaner.rsvr_inflow_cleaner.ReservoirInflowBacktrack.rsvr_inflow_clean--parameters","title":"Parameters","text":"<p>zw_curve_std_times : float     the times of standard deviation to remove outliers, by default 3.0 remove_zw_outliers : bool     whether to remove outliers for z-w curve fitting, by default False</p>"},{"location":"api/#streamflowcleaner","title":"StreamflowCleaner","text":""},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner","title":"<code>hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner</code>","text":"<p>               Bases: <code>Cleaner</code></p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.FFT","title":"<code>FFT(streamflow_data)</code>","text":"<p>\u5bf9\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u7684\u5085\u91cc\u53f6\u6ee4\u6ce2\u5904\u7406\uff0c\u5305\u62ec\u975e\u8d1f\u503c\u8c03\u6574\u548c\u6d41\u91cf\u603b\u91cf\u8c03\u6574\u3002 :cutoff_frequency: \u5085\u91cc\u53f6\u6ee4\u6ce2\u7684\u622a\u6b62\u9891\u7387\u3002 :time_step: \u6570\u636e\u91c7\u6837\u95f4\u9694\u3002 :iterations: \u8fed\u4ee3\u6b21\u6570\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.data_balanced","title":"<code>data_balanced(origin_data, transform_data)</code>","text":"<p>\u5bf9\u4e00\u7ef4\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u603b\u91cf\u5e73\u8861\u53d8\u6362\u3002 :origin_data: \u539f\u59cb\u4e00\u7ef4\u6d41\u91cf\u6570\u636e\u3002 :transform_data: \u5e73\u6ed1\u8f6c\u6362\u540e\u7684\u4e00\u7ef4\u6d41\u91cf\u6570\u636e\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.kalman_filter","title":"<code>kalman_filter(streamflow_data)</code>","text":"<p>\u5bf9\u6d41\u91cf\u6570\u636e\u5e94\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u5e73\u6ed1\u5904\u7406\uff0c\u5e76\u4fdd\u6301\u6d41\u91cf\u603b\u91cf\u5e73\u8861\u3002 :param streamflow_data: \u539f\u59cb\u6d41\u91cf\u6570\u636e</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.lowpass_filter","title":"<code>lowpass_filter(streamflow_data)</code>","text":"<p>\u5bf9\u4e00\u7ef4\u6d41\u91cf\u6570\u636e\u5e94\u7528\u8c03\u6574\u540e\u7684\u4f4e\u901a\u6ee4\u6ce2\u5668\u3002 :cutoff_frequency: \u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u622a\u6b62\u9891\u7387\u3002 :sampling_rate: \u6570\u636e\u7684\u91c7\u6837\u7387\u3002 :order: \u6ee4\u6ce2\u5668\u7684\u9636\u6570\uff0c\u9ed8\u8ba4\u4e3a5\u3002</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.moving_average","title":"<code>moving_average(streamflow_data)</code>","text":"<p>\u5bf9\u6d41\u91cf\u6570\u636e\u5e94\u7528\u6ed1\u52a8\u5e73\u5747\u8fdb\u884c\u5e73\u6ed1\u5904\u7406\uff0c\u5e76\u4fdd\u6301\u6d41\u91cf\u603b\u91cf\u5e73\u8861\u3002 :param streamflow_data: \u8f93\u5165\u7684\u6d41\u91cf\u6570\u636e\u6570\u7ec4 :return: \u5e73\u6ed1\u5904\u7406\u540e\u7684\u6d41\u91cf\u6570\u636e</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.moving_average_difference","title":"<code>moving_average_difference(streamflow_data)</code>","text":"<p>\u5bf9\u6d41\u91cf\u6570\u636e\u5e94\u7528\u6ed1\u52a8\u5e73\u5747\u5dee\u7b97\u6cd5\u8fdb\u884c\u5e73\u6ed1\u5904\u7406\uff0c\u5e76\u4fdd\u6301\u6d41\u91cf\u603b\u91cf\u5e73\u8861\u3002 :window_size: \u6ed1\u52a8\u7a97\u53e3\u7684\u5927\u5c0f</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.robust_fitting","title":"<code>robust_fitting(streamflow_data, k=1.5)</code>","text":"<p>\u5bf9\u6d41\u91cf\u6570\u636e\u5e94\u7528\u6297\u5dee\u4fee\u6b63\u7b97\u6cd5\u8fdb\u884c\u5e73\u6ed1\u5904\u7406\uff0c\u5e76\u4fdd\u6301\u6d41\u91cf\u603b\u91cf\u5e73\u8861\u3002 \u9ed8\u8ba4\u91c7\u7528\u4e8c\u6b21\u66f2\u7ebf\u8fdb\u884c\u62df\u5408\u4f18\u5316\uff0c\u8be5\u7b97\u6cd5\u5904\u7406\u6027\u80fd\u8f83\u5dee</p>"},{"location":"api/#hydrodatasource.cleaner.streamflow_cleaner.StreamflowCleaner.wavelet","title":"<code>wavelet(streamflow_data)</code>","text":"<p>\u5bf9\u4e00\u7ef4\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u5c0f\u6ce2\u53d8\u6362\u5206\u6790\u524d\u540e\u62d3\u5c55\u6570\u636e\u4ee5\u51cf\u5c11\u8fb9\u7f18\u5931\u771f\uff0c\u7136\u540e\u8c03\u6574\u603b\u6d41\u91cf\u3002 :cwt_row: \u5c0f\u6ce2\u53d8\u6362\u4e2d\u4f7f\u7528\u7684\u7279\u5b9a\u5bbd\u5ea6\u3002</p>"},{"location":"api/#waterlevelcleaner","title":"WaterlevelCleaner","text":""},{"location":"api/#hydrodatasource.cleaner.waterlevel_cleaner.WaterlevelCleaner","title":"<code>hydrodatasource.cleaner.waterlevel_cleaner.WaterlevelCleaner</code>","text":"<p>               Bases: <code>Cleaner</code></p>"},{"location":"cleaner/","title":"Cleaner","text":"<p>The <code>cleaner</code> module provides a suite of tools for cleaning and preprocessing raw hydrological time series data. Raw data from gauging stations often contains errors, gaps, and noise. This module helps to identify and correct these issues to prepare the data for analysis and modeling.</p>"},{"location":"cleaner/#rainfall-cleaner","title":"Rainfall Cleaner","text":"<p>File: <code>rainfall_cleaner.py</code></p> <p><code>RainfallCleaner</code> is used to validate and clean station-based rainfall data.</p> <ul> <li>Yearly Check: It compares the total annual rainfall from a station against a reference dataset (like ERA5-Land reanalysis) to identify stations that may be unreliable.</li> <li>Extreme Value Check: It flags and removes precipitation values that are physically unrealistic (e.g., 200 mm in a single hour).</li> <li>Time Series Check: It detects anomalies like sudden, sharp gradients in the data or periods where the sensor appears to be stuck on a constant low value.</li> </ul>"},{"location":"cleaner/#reservoir-inflow-cleaner","title":"Reservoir Inflow Cleaner","text":"<p>File: <code>rsvr_inflow_cleaner.py</code></p> <p><code>ReservoirInflowBacktrack</code> is a powerful tool for estimating reservoir inflow when direct measurements are not available. It uses the water balance method, based on reservoir water level, storage capacity, and outflow.</p> <ol> <li>Clean Storage Data: It first cleans the reservoir water level and storage data, removing outliers and fitting a water level-storage (Z-W) curve to ensure consistency.</li> <li>Back-calculate Inflow: It then calculates inflow using the principle: <code>Inflow = Outflow + Change in Storage</code>.</li> <li>Correct Negative Values: Since negative inflow is physically impossible, it adjusts these values while preserving the overall water balance.</li> <li>Interpolate: Finally, it interpolates the data to a consistent hourly time step.</li> </ol>"},{"location":"cleaner/#streamflow-cleaner","title":"Streamflow Cleaner","text":"<p>File: <code>streamflow_cleaner.py</code></p> <p><code>StreamflowCleaner</code> focuses on smoothing noisy streamflow data. This is often necessary to reduce measurement noise without distorting the underlying hydrological signal.</p> <p>It offers several smoothing algorithms, including: - Simple Moving Average (<code>moving_average</code>) - Kalman Filter (<code>kalman_filter</code>) - Low-pass Butterworth Filter (<code>lowpass_filter</code>) - Fast Fourier Transform (<code>FFT</code>) and Wavelet (<code>wavelet</code>) filtering</p> <p>All methods are designed to be volume-preserving, meaning the total volume of streamflow is not changed by the smoothing process.</p>"},{"location":"cleaner/#water-level-cleaner","title":"Water Level Cleaner","text":"<p>File: <code>waterlevel_cleaner.py</code></p> <p><code>WaterlevelCleaner</code> is designed to fix anomalies in water level data.</p> <ul> <li>Gradient Filter: Its main feature is a <code>moving_gradient_filter</code> that identifies and removes data points where the water level changes at an unrealistic rate. It uses different thresholds for flood seasons and non-flood seasons to avoid removing legitimate rapid changes during high-flow events.</li> <li>Filling Gaps: It also provides a <code>rolling_fill</code> method to fill in missing data points based on the most frequent value (mode) in a moving window.</li> </ul>"},{"location":"configs/","title":"Configuration","text":"<p><code>hydrodatasource</code> uses a central YAML configuration file to manage data paths and connections to remote services. This makes it easy to adapt the library to your local environment.</p>"},{"location":"configs/#the-hydro_settingyml-file","title":"The <code>hydro_setting.yml</code> File","text":"<p>The configuration is managed through a file named <code>hydro_setting.yml</code> located in your user home directory (e.g., <code>C:\\Users\\YourUser</code> on Windows or <code>/home/YourUser</code> on Linux).</p> <p>If this file does not exist, <code>hydrodatasource</code> will automatically create and use a default directory named <code>hydrodatasource_data</code> in your home directory.</p>"},{"location":"configs/#local-data-path-configuration","title":"Local Data Path Configuration","text":"<p>For most use cases, you only need to configure the <code>local_data_path</code> section. This tells the library where to find and store your data.</p> <p>Here is an example of a minimal <code>hydro_setting.yml</code>:</p> <pre><code>local_data_path:\n  root: 'D:\\data\\hydro_data' # The main directory for all your data\n  datasets-origin: 'D:\\data\\hydro_data\\origin' # For raw, unprocessed data\n  datasets-interim: 'D:\\data\\hydro_data\\interim' # For intermediate, processed data\n  cache: 'D:\\data\\hydro_data\\.cache' # For storing cached files like NetCDFs\n</code></pre> <ul> <li><code>root</code>: The top-level directory for all project-related data.</li> <li><code>datasets-origin</code>: This is where you should place your original, raw datasets.</li> <li><code>datasets-interim</code>: This directory is used for storing data that has been processed or transformed in some way.</li> <li><code>cache</code>: <code>hydrodatasource</code> uses this directory to store cached data, such as the NetCDF files generated by <code>SelfMadeHydroDataset</code>. This speeds up data loading on subsequent runs.</li> </ul>"},{"location":"configs/#other-configurations","title":"Other Configurations","text":"<p>The <code>hydro_setting.yml</code> file can also be used to configure connections to a MinIO object storage server and a PostgreSQL database, but these are not required for basic local file-based operations.</p>"},{"location":"processor/","title":"Processor","text":"<p>The <code>processor</code> module contains functions for advanced processing of hydrological data. This includes spatial analysis, like calculating basin-average rainfall, and time series analysis, like identifying distinct rainfall-runoff events.</p>"},{"location":"processor/#basin-mean-rainfall","title":"Basin Mean Rainfall","text":"<p>When working with multiple rainfall gauges in a basin, you often need to calculate a single, representative rainfall value for the entire basin. The <code>basin_mean_rainfall.py</code> module provides tools for this.</p> <ul> <li><code>calculate_thiesen_polygons</code>: This function generates Thiessen polygons from station locations. Each polygon represents the area that is closest to a particular station, and the area of these polygons can be used to weight the station's rainfall data.</li> <li><code>basin_mean_func</code>: This is the main function for calculating the basin's mean rainfall. It can perform either a simple arithmetic average or a weighted average using the weights derived from the Thiessen polygons.</li> </ul>"},{"location":"processor/#example-usage","title":"Example Usage","text":"<pre><code>import geopandas as gpd\nimport pandas as pd\nfrom hydrodatasource.processor.basin_mean_rainfall import calculate_thiesen_polygons, basin_mean_func\n\n# Load station locations and basin boundary\nstations_gdf = gpd.read_file(\"path/to/stations.shp\")\nbasin_gdf = gpd.read_file(\"path/to/basin.shp\")\n\n# Load rainfall data (as a DataFrame with station IDs as columns)\n# Make sure the columns are sorted alphabetically\nrainfall_df = pd.read_csv(\"path/to/rainfall.csv\", index_col=\"time\")\nrainfall_df = rainfall_df.sort_index(axis=1)\n\n# Calculate Thiessen polygons to get station weights\nweights_gdf = calculate_thiesen_polygons(stations_gdf, basin_gdf)\n\n# Create a weights dictionary\nweights_dict = {\n    tuple(weights_gdf[\"STCD\"]): weights_gdf[\"area_ratio\"].tolist()\n}\n\n# Calculate the basin mean rainfall\nmean_rainfall = basin_mean_func(rainfall_df, weights_dict=weights_dict)\n\nprint(mean_rainfall)\n</code></pre>"},{"location":"processor/#rainfall-runoff-event-identification","title":"Rainfall-Runoff Event Identification (\u573a\u6b21\u5212\u5206)","text":"<p>The <code>dmca_esr.py</code> module implements the DMCA-ESR method for identifying and separating individual rainfall-runoff events from continuous time series data. This is crucial for event-based hydrological modeling and analysis.</p> <ul> <li><code>get_rr_events</code>: This is the primary function to use. It takes rainfall and streamflow data (as xarray DataArrays) and returns a dictionary where each key is a basin ID and the value is a pandas DataFrame listing the identified events.</li> </ul>"},{"location":"processor/#example-usage_1","title":"Example Usage","text":"<pre><code>import xarray as xr\nfrom hydrodatasource.processor.dmca_esr import get_rr_events\n\n# Assume 'rain_da' and 'flow_da' are xarray DataArrays with dimensions ('time', 'basin')\n# and 'basin_area' is an xarray Dataset with the area for each basin.\nrain_da = xr.open_dataset(\"path/to/rain.nc\")[\"precipitation\"]\nflow_da = xr.open_dataset(\"path/to/flow.nc\")[\"streamflow\"]\nbasin_area = xr.open_dataset(\"path/to/attributes.nc\")[\"area\"]\n\n# Identify rainfall-runoff events\nrr_events = get_rr_events(rain_da, flow_da, basin_area)\n\n# Print the events for a specific basin\nfor basin_id, events_df in rr_events.items():\n    print(f\"Events for basin {basin_id}:\")\n    print(events_df)\n</code></pre>"},{"location":"reader/","title":"Reader","text":"<p>The <code>reader</code> module is the core component of <code>hydrodatasource</code> for accessing and reading various hydrological datasets. It provides a unified interface for handling different data sources, with a special focus on custom, user-prepared datasets.</p>"},{"location":"reader/#selfmadehydrodataset","title":"SelfMadeHydroDataset","text":"<p>The <code>SelfMadeHydroDataset</code> class is the most important feature of the <code>reader</code> module. It allows you to read your own hydrological data as long as it follows a specific directory structure. This is designed for flexibility, enabling you to work with non-public or specially prepared datasets.</p>"},{"location":"reader/#directory-structure","title":"Directory Structure","text":"<p>To use <code>SelfMadeHydroDataset</code>, your data should be organized in the following structure:</p> <pre><code>/path/to/your_dataset_name/\n\u251c\u2500\u2500 attributes/\n\u2502   \u251c\u2500\u2500 attributes.csv\n\u251c\u2500\u2500 shapes/\n\u2502   \u251c\u2500\u2500 basins.shp\n\u251c\u2500\u2500 timeseries/\n\u2502   \u251c\u2500\u2500 1D/\n\u2502   \u2502   \u251c\u2500\u2500 basin_1.csv\n\u2502   \u2502   \u251c\u2500\u2500 basin_2.csv\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 1D_units_info.json\n\u2502   \u251c\u2500\u2500 3h/\n\u2502   \u2502   \u251c\u2500\u2500 basin_1.csv\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 3h_units_info.json\n</code></pre> <ul> <li><code>attributes/attributes.csv</code>: A CSV file containing static attributes for each basin (e.g., area, slope, land cover). It must contain a <code>basin_id</code> column.</li> <li><code>shapes/basins.shp</code>: A shapefile containing the geographic boundaries of each basin.</li> <li><code>timeseries/</code>: This directory holds the time series data, with subdirectories for each time resolution (e.g., <code>1D</code> for daily, <code>3h</code> for 3-hourly).<ul> <li>Each subdirectory contains CSV files, one for each basin, named with the <code>basin_id</code>.</li> <li>Each subdirectory also contains a <code>*_units_info.json</code> file that specifies the units for the variables in the CSV files.</li> </ul> </li> </ul>"},{"location":"reader/#example-usage","title":"Example Usage","text":"<p>Here is how you can use <code>SelfMadeHydroDataset</code> to read your data:</p> <pre><code>from hydrodatasource.reader.data_source import SelfMadeHydroDataset\n\n# Path to the parent directory of your dataset\ndata_path = \"/path/to/your_data/\"\n# The name of your dataset directory\ndataset_name = \"my_custom_dataset\"\n\n# Initialize the reader\nreader = SelfMadeHydroDataset(data_path=data_path, dataset_name=dataset_name, time_unit=[\"1D\"])\n\n# Get a list of all basin IDs\nbasin_ids = reader.read_object_ids()\n\n# Define the time range and variables to read\nt_range = [\"2000-01-01\", \"2010-12-31\"]\nvariables = [\"precipitation\", \"streamflow\"]\n\n# Read the time series data\ntimeseries_data = reader.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=t_range,\n    var_lst=variables,\n    time_units=[\"1D\"]\n)\n\n# The result is a dictionary with time units as keys and xarray.Dataset as values\ndaily_data = timeseries_data[\"1D\"]\nprint(daily_data)\n</code></pre>"},{"location":"reader/#other-readers","title":"Other Readers","text":"<ul> <li><code>SelfMadeForecastDataset</code>: Extends <code>SelfMadeHydroDataset</code> to support forecast data, which is expected to be in a <code>forecasts</code> directory.</li> <li><code>StationHydroDataset</code>: Extends <code>SelfMadeHydroDataset</code> to include data from gauging stations, which is expected to be in a <code>stations</code> directory.</li> </ul>"}]}